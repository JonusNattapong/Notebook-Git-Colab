#  AI/LLM Learning Resources 2025 by zombitx64

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*

*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

‡∏Ñ‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏î‡πâ‡∏≤‡∏ô Artificial Intelligence (AI) ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô](#‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
   - ‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning
   - Python ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI
   - Neural Networks
   - Natural Language Processing (NLP)
2. [‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô](#‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)
   - Notebooks ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
   - ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
3. [‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£](#‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£)
   - Datasets
   - Tools
   - ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

## ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

---

### üìö LLM Fundamentals

#### 1. Mathematics for Machine Learning
‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á AI ‡πÅ‡∏•‡∏∞ Machine Learning (ML) ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•  
- **Linear Algebra**:  
  - ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° Vectors, Matrices, Derivatives, Integrals, Limits, Series, Multivariable Calculus ‡πÅ‡∏•‡∏∞ Gradient Concepts  
  - ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deep Learning ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏ô Neural Networks  
- **Probability and Statistics**:  
  - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢  
  - ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô Probability Distributions, Hypothesis Testing, Bayesian Inference  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Mathematics for Machine Learning](https://mml-book.github.io/) - ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ü‡∏£‡∏µ‡∏à‡∏≤‡∏Å Cambridge University Press  
  - [3Blue1Brown - Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) - ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏™‡∏≠‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢  
  - [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability) - ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏ü‡∏£‡∏µ  

#### 2. Python for AI
Python ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡πÅ‡∏•‡∏∞ LLMs ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ Libraries ‡∏ó‡∏µ‡πà‡∏ó‡∏£‡∏á‡∏û‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢  
- **‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Python**: Variables, Functions, Loops, Data Structures (Lists, Dictionaries)  
- **Libraries ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**:  
  - *NumPy*: ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç  
  - *Pandas*: ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
  - *Matplotlib*: ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÇ‡∏î‡∏¢ Jake VanderPlas ‡∏û‡∏£‡πâ‡∏≠‡∏° Notebooks  
  - [RealPython](https://realpython.com/) - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô Python ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î  
  - [CS231n Python Tutorial](https://cs231n.github.io/python-numpy-tutorial/) - ‡∏à‡∏≤‡∏Å Stanford  

#### 3. Neural Networks
Neural Networks ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á Deep Learning ‡πÅ‡∏•‡∏∞ LLMs ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡πÑ‡∏õ‡∏™‡∏π‡πà Transformer Models  
- **‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô**:  
  - Layers, Weights, Biases, Activation Functions (Sigmoid, Tanh, ReLU)  
  - ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Feedforward ‡πÅ‡∏•‡∏∞ Backpropagation  
- **‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•**:  
  - Loss Functions, Optimization (Gradient Descent), Overfitting Prevention  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ü‡∏£‡∏µ‡πÇ‡∏î‡∏¢ Michael Nielsen  
  - [CS231n: Convolutional Neural Networks](https://cs231n.github.io/) - ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏à‡∏≤‡∏Å Stanford  
  - [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) - ‡πÇ‡∏î‡∏¢ Andrew NG ‡∏ö‡∏ô Coursera  

#### 4. Natural Language Processing (NLP)
NLP ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£ ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á LLMs  
- **Text Preprocessing**:  
  - *Tokenization*: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ  
  - *Stemming/Lemmatization*: ‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏£‡∏≤‡∏Å  
  - *Stop Word Removal*: ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç  
- **Feature Extraction Techniques**:  
  - *Bag-of-Words (BoW)*: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ñ‡∏≥  
  - *TF-IDF*: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç  
  - *N-grams*: ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á  
- **Word Embeddings**:  
  - *Word2Vec, GloVe, FastText*: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô  
- **Recurrent Neural Networks (RNNs)**:  
  - ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏≥‡∏î‡∏±‡∏ö (Sequential Data)  
  - Variants ‡πÄ‡∏ä‡πà‡∏ô LSTMs ‡πÅ‡∏•‡∏∞ GRUs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö Long-Term Dependencies  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html) - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Word Embeddings  
  - [RealPython - NLP with spaCy](https://realpython.com/natural-language-processing-spacy-python/) - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ spaCy  
  - [Jay Alammar - Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - ‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢  
  - [Colah‚Äôs Blog - Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ LSTMs  
  - [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Kaggle  

---

### üìù Introductory Notebooks
Notebooks ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏£‡∏¥‡∏á‡∏ö‡∏ô‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏° ‡πÄ‡∏ä‡πà‡∏ô Google Colab ‡πÅ‡∏•‡∏∞ Kaggle

#### 1. Unsloth Notebooks
- **‡∏ó‡∏µ‡πà‡∏°‡∏≤**: [GitHub: unslothai/notebooks](https://github.com/unslothai/notebooks)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏£‡∏ß‡∏° Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÅ‡∏•‡∏∞ Inference ‡πÇ‡∏°‡πÄ‡∏î‡∏• LLMs ‡∏ö‡∏ô Google Colab ‡πÅ‡∏•‡∏∞ Kaggle  
- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
  - **GRPO Notebooks**:  
    - [Phi 4 (14B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)  
    - [Llama 3.1 (8B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)  
  - **Llama Notebooks**:  
    - [Llama 3.2 (1B and 3B) - Conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)  
    - [Llama 3.2 (11B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)  
  - **Mistral Notebooks**:  
    - [Mistral Small (22B) - Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb)  
    - [Mistral (7B) - Text Completion](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)  
  - **Kaggle Variants**: ‡∏°‡∏µ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Kaggle ‡πÄ‡∏ä‡πà‡∏ô [Kaggle-Llama3.1_(8B)-Alpaca](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Llama3.1_(8B)-Alpaca.ipynb)  

#### 2. Origins AI Notebooks
- **‡∏ó‡∏µ‡πà‡∏°‡∏≤**: [OriginsHQ](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Notebooks ‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ LLMs  
- **Tools**:  
  - [üßê LLM AutoEval](https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa) - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô LLMs ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏î‡πâ‡∏ß‡∏¢ RunPod  
  - [ü•± LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb) - ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ MergeKit  
  - [ü¶é LazyAxolotl](https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW) - Fine-Tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô Cloud  
  - [‚ö° AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4) - Quantize LLMs ‡πÄ‡∏õ‡πá‡∏ô GGUF, GPTQ  
- **Fine-Tuning**:  
  - [Fine-tune Llama 3.1 with Unsloth](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z) - ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°: [Link](https://originshq.com/blog/fine-tune-llama-3-1-ultra-efficiently-with-unsloth/)  
  - [Fine-tune Mistral-7b with QLoRA](https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS) - ‡πÉ‡∏ä‡πâ TRL  

#### 3. Awesome Colab Notebooks
- **‡∏ó‡∏µ‡πà‡∏°‡∏≤**: [GitHub: amrzv/awesome-colab-notebooks](https://github.com/amrzv/awesome-colab-notebooks)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML Experiments  
- **Courses**:  
  - [ARENA](https://colab.research.google.com/drive/1vuQOB2Gd7OcfzH2y9djXm9OdZA_DcxYz) - ML Engineering ‡πÇ‡∏î‡∏¢ Callum McDougall  
  - [Autodiff Cookbook](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb) - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Autodifferentiation  
  - [Machine Learning Simplified](https://colab.research.google.com/github/5x12/themlsbook/blob/master/chapter2/knn.ipynb) - ‡πÇ‡∏î‡∏¢ Andrew Wolf  
  - [Deep RL Course](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb) - ‡∏à‡∏≤‡∏Å Hugging Face  
- **Research**:  
  - [AlphaFold](https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb) - Protein Structure Prediction  

---

### üéì Online Courses and Tutorials
‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡πÅ‡∏•‡∏∞ Tutorials ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô  
- **Andrew NG Machine Learning Course** ([Coursera](https://www.coursera.org/learn/machine-learning)) - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ML  
- **Deep Learning Specialization** ([Coursera](https://www.coursera.org/specializations/deep-learning)) - 5 ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏à‡∏≤‡∏Å Andrew NG  
- **NYU-DLSP20** ([GitHub](https://github.com/Atcold/NYU-DLSP20)) - Deep Learning ‡πÇ‡∏î‡∏¢ Yann LeCun  
- **mlcourse.ai** ([GitHub](https://github.com/Yorko/mlcourse.ai)) - Open ML Course ‡πÇ‡∏î‡∏¢ Yury Kashnitsky  

---

### üì¶ Datasets and Tools
- **Kaggle Datasets** ([Kaggle](https://www.kaggle.com/datasets)) - ‡∏Ñ‡∏•‡∏±‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô  
- **NLP Datasets** ([GitHub](https://github.com/awwsmm/nlp-datasets)) - 100+ ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NLP  
- **Hugging Face Transformers** ([GitHub](https://github.com/huggingface/transformers)) - Library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning BERT, GPT  

---

### üéôÔ∏è Additional Learning Resources
- **Podcasts**:  
  - [Lex Fridman Podcast](https://lexfridman.com/podcast/) - ‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç AI  
  - [Data Skeptic](https://dataskeptic.com/) - Data Science ‡πÅ‡∏•‡∏∞ ML  
- **YouTube Channels**:  
  - [Sentdex](https://www.youtube.com/@sentdex) - Tutorials ML  
  - [Corey Schafer](https://www.youtube.com/@coreyschafer) - Python Coding  
- **Communities**:  
  - [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) - ‡∏ä‡∏∏‡∏°‡∏ä‡∏ô ML ‡∏ö‡∏ô Reddit  
  - [Discord - Unsloth](https://discord.gg/unsloth) - ‡∏ä‡∏∏‡∏°‡∏ä‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö  

---

### üöÄ How to Get Started
1. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠**: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Mathematics ‡∏´‡∏£‡∏∑‡∏≠ Python ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏ô‡∏±‡∏î  
2. **‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢ Notebooks**: ‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô Colab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥  
3. **‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏Ñ‡∏≠‡∏£‡πå‡∏™**: ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏ü‡∏£‡∏µ ‡πÄ‡∏ä‡πà‡∏ô Andrew NG ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ  

# Awesome AI/LLM Learning Resources for 2025 (Part 2/4)

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*  
*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

## Part 2: The LLM Scientist - Advanced LLM Development

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á LLMs ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°, ‡∏Å‡∏≤‡∏£ Pre-training, Post-training, Fine-Tuning, Preference Alignment, ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•, Quantization ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡πÉ‡∏´‡∏°‡πà ‡πÜ

---

### üß† 1. LLM Architecture
‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á LLMs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•  
- **Architectural Overview**:  
  - Evolution ‡∏à‡∏≤‡∏Å *Encoder-Decoder Transformers* (‡πÄ‡∏ä‡πà‡∏ô BERT) ‡∏™‡∏π‡πà *Decoder-Only* (‡πÄ‡∏ä‡πà‡∏ô GPT)  
  - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á  
- **Tokenization**:  
  - ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Numerical Tokens)  
  - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ ‡πÄ‡∏ä‡πà‡∏ô Byte-Pair Encoding (BPE), WordPiece  
- **Attention Mechanisms**:  
  - *Self-Attention*: ‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°  
  - Variants ‡πÄ‡∏ä‡πà‡∏ô Multi-Head Attention, Long-Range Dependencies  
- **Sampling Techniques**:  
  - *Deterministic*: Greedy Search, Beam Search  
  - *Probabilistic*: Temperature Sampling, Nucleus Sampling  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [3Blue1Brown - Visual Intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Transformer ‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏û  
  - [Andrej Karpathy - nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) - ‡∏™‡∏£‡πâ‡∏≤‡∏á GPT ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å (‡∏°‡∏µ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ Tokenization: [Link](https://www.youtube.com/watch?v=zduSFxRajkE))  
  - [Lilian Weng - Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) - ‡∏Å‡∏•‡πÑ‡∏Å Attention  
  - [Maxime Labonne - Decoding Strategies](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°  

---

### ‚öôÔ∏è 2. Pre-training Models
Pre-training ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏π‡∏á ‡πÅ‡∏ï‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô  
- **Data Preparation**:  
  - ‡πÉ‡∏ä‡πâ Dataset ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (‡πÄ‡∏ä‡πà‡∏ô Llama 3.1 ‡∏ù‡∏∂‡∏Å‡∏ö‡∏ô 15T Tokens)  
  - ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô: Curate, Clean, Deduplicate, Tokenize, Quality Filtering  
- **Distributed Training**:  
  - *Data Parallelism*: ‡πÅ‡∏ö‡πà‡∏á Batch ‡πÑ‡∏õ‡∏¢‡∏±‡∏á GPU ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß  
  - *Pipeline Parallelism*: ‡πÅ‡∏ö‡πà‡∏á Layers  
  - *Tensor Parallelism*: ‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì  
- **Training Optimization**:  
  - Adaptive Learning Rates, Gradient Clipping, Mixed-Precision Training  
  - Optimizers: AdamW, Lion  
- **Monitoring**:  
  - ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° Loss, Gradients, GPU Usage ‡∏î‡πâ‡∏ß‡∏¢ Dashboards  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) - Dataset ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÇ‡∏î‡∏¢ Penedo et al.  
  - [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2) - Dataset ‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏î‡∏¢ Weber et al.  
  - [Nanotron](https://github.com/huggingface/nanotron) - ‡πÉ‡∏ä‡πâ‡∏ù‡∏∂‡∏Å SmolLM2 ([Link](https://github.com/huggingface/smollm))  
  - [Parallel Training](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf) - ‡πÇ‡∏î‡∏¢ Chenyan Xiong  
  - [Distributed Training](https://arxiv.org/abs/2407.20018) - Paper ‡πÇ‡∏î‡∏¢ Duan et al.  
  - [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor) - Nested Data Parallelism  

---

### üìä 3. Post-training Datasets
Post-training ‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô  
- **Storage & Chat Templates**:  
  - Formats: ShareGPT, OpenAI/HF  
  - Chat Templates: ChatML, Alpaca  
- **Synthetic Data Generation**:  
  - ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á (‡πÄ‡∏ä‡πà‡∏ô GPT-4o) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏π‡πà Instruction-Response  
  - ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: Diverse Seed Tasks, Effective Prompts  
- **Data Enhancement**:  
  - Verified Outputs (Unit Tests), Rejection Sampling, Auto-Evol ([Paper](https://arxiv.org/abs/2406.00770))  
  - Chain-of-Thought, Branch-Solve-Merge, Persona-based  
- **Quality Filtering**:  
  - Rule-based, Duplicate Removal (MinHash/Embeddings), N-gram Decontamination  
  - ‡πÉ‡∏ä‡πâ Reward Models ‡πÅ‡∏•‡∏∞ Judge LLMs  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LLM Datasets](https://github.com/mlabonne/llm-datasets) - ‡∏Ñ‡∏•‡∏±‡∏á Dataset ‡πÇ‡∏î‡∏¢ Maxime Labonne  
  - [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator) - ‡πÇ‡∏î‡∏¢ Argilla  
  - [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator) - ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Dataset  
  - [Distilabel](https://distilabel.argilla.io/dev/sections/pipeline_samples/) - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û  
  - [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Hugging Face  

---

### üîß 4. Supervised Fine-Tuning (SFT)
SFT ‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ  
- **Training Techniques**:  
  - *Full Fine-Tuning*: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏∏‡∏Å‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏π‡∏á)  
  - *LoRA*: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Adapter Parameters ‡πÄ‡∏â‡∏û‡∏≤‡∏∞  
  - *QLoRA*: ‡∏£‡∏ß‡∏° 4-bit Quantization ‡∏Å‡∏±‡∏ö LoRA  
- **Training Parameters**:  
  - Learning Rate (‡∏Å‡∏±‡∏ö Schedulers), Batch Size, Gradient Accumulation  
  - Optimizers: 8-bit AdamW, Weight Decay, Warmup Steps  
  - LoRA Parameters: Rank, Alpha, Target Modules  
- **Distributed Training**:  
  - DeepSpeed (ZeRO Optimization), FSDP, Gradient Checkpointing  
- **Monitoring**:  
  - Loss Curves, Learning Rate Changes, Gradient Norms  
- **Notebooks**:  
  - [Fine-tune Llama 3.1 with Unsloth](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z) - [Article](https://originshq.com/blog/fine-tune-llama-3-1-ultra-efficiently-with-unsloth/)  
  - [Fine-tune Mistral-7b with QLoRA](https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS) - ‡πÉ‡∏ä‡πâ TRL  
  - [Llama 3.1 (8B) - Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Fine-tune Llama 3.1 with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) - ‡πÇ‡∏î‡∏¢ Maxime Labonne  
  - [Axolotl Documentation](https://axolotl-ai-cloud.github.io/axolotl/) - ‡πÇ‡∏î‡∏¢ Wing Lian  
  - [LoRA Insights](https://lightning.ai/pages/community/lora-insights/) - ‡πÇ‡∏î‡∏¢ Sebastian Raschka  
  - [QLoRA Fine-Tuning](https://github.com/georgesung/llm_qlora/blob/main/train.py) - Script  

---

### üéØ 5. Preference Alignment
‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô Toxicity, Hallucinations  
- **Rejection Sampling**:  
  - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡πà‡∏≠ Prompt ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å/‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò  
- **Direct Preference Optimization (DPO)**:  
  - ‡πÄ‡∏û‡∏¥‡πà‡∏° Likelihood ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ([Paper](https://arxiv.org/abs/2305.18290))  
- **Proximal Policy Optimization (PPO)**:  
  - ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Policy ‡∏î‡πâ‡∏ß‡∏¢ Reward Model ([Paper](https://arxiv.org/abs/1707.06347))  
- **Monitoring**:  
  - Margin ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Chosen/Rejected Responses, Accuracy  
- **Notebooks**:  
  - [Fine-tune Mistral-7b with DPO](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE) - [Article](https://originshq.com/blog/boost-the-performance-of-supervised-fine-tuned-models-with-dpo/)  
  - [Fine-tune Llama 3 with ORPO](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi) - [Article](https://originshq.com/blog/fine-tune-llama-3-with-orpo/)  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Illustrating RLHF](https://huggingface.co/blog/rlhf) - ‡πÇ‡∏î‡∏¢ Hugging Face  
  - [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  
  - [DPO Wandb Logs](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4) - ‡πÇ‡∏î‡∏¢ Alexander Vishnevskiy  

---

### üìà 6. Evaluation
‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• LLMs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Dataset ‡πÅ‡∏•‡∏∞ Training  
- **Automated Benchmarks**:  
  - ‡πÉ‡∏ä‡πâ MMLU, TriviaQA ‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û  
- **Human Evaluation**:  
  - Community Voting (‡πÄ‡∏ä‡πà‡∏ô Arena), Subjective Assessments  
- **Model-based Evaluation**:  
  - Judge Models, Reward Models  
- **Feedback Signal**:  
  - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Error Patterns ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook) - ‡πÇ‡∏î‡∏¢ Cl√©mentine Fourrier  
  - [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) - ‡πÇ‡∏î‡∏¢ Hugging Face  
  - [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) - ‡πÇ‡∏î‡∏¢ EleutherAI  
  - [Chatbot Arena](https://lmarena.ai/) - ‡πÇ‡∏î‡∏¢ LMSYS  

---

### ‚ö° 7. Quantization
‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡∏ô Hardware ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ  
- **Base Techniques**:  
  - Precisions: FP32, FP16, INT8, 4-bit  
  - Methods: Absmax, Zero-point  
- **GGUF & llama.cpp**:  
  - ‡∏£‡∏±‡∏ô‡∏ö‡∏ô CPU/GPU ‡∏î‡πâ‡∏ß‡∏¢ [llama.cpp](https://github.com/ggerganov/llama.cpp)  
- **GPTQ & AWQ**:  
  - Layer-wise Calibration ([GPTQ Paper](https://arxiv.org/abs/2210.17323), [AWQ Paper](https://arxiv.org/abs/2306.00978))  
- **SmoothQuant & ZeroQuant**:  
  - ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô Quantization  
- **Notebooks**:  
  - [4-bit Quantization with GPTQ](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A) - [Article](https://originshq.com/blog/4-bit-llm-quantization-with-gptq/)  
  - [Quantization with GGUF](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD) - [Article](https://originshq.com/blog/quantize-llama-models-with-gguf-and-llama-cpp/)  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Introduction to Quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) - ‡πÇ‡∏î‡∏¢ Maxime Labonne  
  - [DeepSpeed Model Compression](https://www.deepspeed.ai/tutorials/model-compression/) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  

---

### üåü 8. New Trends
‡∏™‡∏≥‡∏£‡∏ß‡∏à‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÉ‡∏ô‡∏ß‡∏á‡∏Å‡∏≤‡∏£ LLMs  
- **Model Merging**:  
  - ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ [Mergekit](https://github.com/cg123/mergekit) (SLERP, DARE, TIES)  
- **Multimodal Models**:  
  - CLIP, LLaVA, Stable Diffusion - ‡∏£‡∏ß‡∏° Text, Image, Audio  
- **Interpretability**:  
  - Sparse Autoencoders (SAEs), Abliteration - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•  
- **Test-time Compute**:  
  - ‡∏õ‡∏£‡∏±‡∏ö Compute ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Inference (‡πÄ‡∏ä‡πà‡∏ô Process Reward Model)  
- **Notebooks**:  
  - [Merge LLMs with Mergekit](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr) - [Article](https://originshq.com/blog/merge-large-language-models-with-mergekit/)  
  - [Uncensor LLM with Abliteration](https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR) - [Article](https://originshq.com/blog/uncensor-any-llm-with-abliteration/)  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Merge LLMs with Mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html) - ‡πÇ‡∏î‡∏¢ Maxime Labonne  
  - [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) - ‡πÇ‡∏î‡∏¢ Chip Huyen  
  - [Intuitive Explanation of SAEs](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) - ‡πÇ‡∏î‡∏¢ Adam Karvonen  
  - [Scaling Test-time Compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) - ‡πÇ‡∏î‡∏¢ Beeching et al.  

---

### üìù Advanced Scripts and Repositories
- **Unsloth**:  
  - [GitHub](https://github.com/unslothai/unsloth) - Tools ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÅ‡∏•‡∏∞ Quantization  
- **ml-systems-papers**:  
  - [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor) - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)  
  - [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain) - Long-Sequence Training  
- **Awesome Colab**:  
  - [ModernBERT](https://colab.research.google.com/github/AnswerDotAI/ModernBERT/blob/master/examples/finetune_modernbert_on_glue.ipynb) - Fine-Tuning Encoder Models  

---

### üöÄ How to Proceed
1. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠**: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Architecture ‡∏´‡∏£‡∏∑‡∏≠ Pre-training  
2. **‡∏ó‡∏î‡∏•‡∏≠‡∏á Notebooks**: ‡∏£‡∏±‡∏ô‡πÉ‡∏ô Colab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥  
3. **‡∏®‡∏∂‡∏Å‡∏©‡∏≤ Papers**: ‡∏≠‡πà‡∏≤‡∏ô Paper ‡πÅ‡∏•‡∏∞ Scripts ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å  

# Awesome AI/LLM Learning Resources for 2025 (Part 3/4)

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*  
*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

## Part 3: The LLM Engineer - Building LLM Applications

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥ LLMs ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•, ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Storage, Retrieval Augmented Generation (RAG), ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á RAG ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á, ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Inference, ‡∏Å‡∏≤‡∏£ Deploy ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•

---

### üöÄ 1. Running LLMs
‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ô LLMs ‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Prompt  
- **Using APIs**:  
  - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö OpenAI, Hugging Face Inference API, Grok API  
  - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢, Scalable  
- **Local Deployment**:  
  - Tools: LM Studio, Ollama, llama.cpp  
  - Hardware: CPU, GPU, Mac M1/M2  
- **Prompt Engineering**:  
  - *Zero-Shot*: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á  
  - *Few-Shot*: ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô Prompt  
  - *Prompt Chaining*: ‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Prompt Engineering Guide](https://www.promptingguide.ai/) - ‡πÇ‡∏î‡∏¢ DAIR.AI  
  - [Run LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  
  - [Hugging Face Inference API](https://huggingface.co/docs/api-inference/quicktour) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£  
  - [Ollama Documentation](https://ollama.ai/docs) - ‡∏£‡∏±‡∏ô Local Models  

---

### üìÇ 2. Building Vector Storage
‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ LLMs ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å  
- **Document Ingestion**:  
  - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå: PDF, JSON, CSV, Markdown  
  - Tools: PyPDF2, pdfplumber  
- **Text Splitting**:  
  - *Recursive Splitting*: ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£, Tokens  
  - *Semantic Splitting*: ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢  
- **Embedding Models**:  
  - *Sentence Transformers*: All-MiniLM-L6-v2, BGE  
  - *OpenAI Embeddings*: text-embedding-ada-002  
- **Vector Databases**:  
  - *Chroma*: Open-source, Local  
  - *Pinecone*: Cloud-based, Scalable  
  - *FAISS*: High-performance Similarity Search  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LangChain - Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  
  - [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Embedding Model Rankings  
  - [Pinecone - Vector Search](https://www.pinecone.io/learn/vector-search/) - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô  
  - [Chroma Docs](https://docs.trychroma.com/) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£  

---

### üîç 3. Retrieval Augmented Generation (RAG)
‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Retrieval) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Generation)  
- **Orchestrators**:  
  - *LangChain*: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Workflow, Memory  
  - *LlamaIndex*: Data Ingestion, Query Engine  
- **Retrievers**:  
  - *Multi-Query*: ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö  
  - *HyDE*: Hypothetical Document Embeddings  
  - *Parent Document Retrieval*: ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î  
- **Evaluation**:  
  - *Ragas*: ‡∏ß‡∏±‡∏î Faithfulness, Relevance  
  - *DeepEval*: Metrics ‡πÄ‡∏ä‡πà‡∏ô BLEU, ROUGE  
- **Notebooks**:  
  - [LangChain RAG](https://colab.research.google.com/drive/1f3VFD6jCSvK0uo2Q84-TT1AbfunN-UEZ) - [Article](https://originshq.com/blog/build-a-retrieval-augmented-generation-rag-app-with-langchain/)  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LangChain - Q&A with RAG](https://python.langchain.com/docs/use_cases/question_answering/quickstart) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  
  - [LlamaIndex Docs](https://docs.llamaindex.ai/en/stable/) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£  
  - [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/) - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô  
  - [Ragas Documentation](https://docs.ragas.io/en/stable/) - Evaluation Framework  

---

### ‚öôÔ∏è 4. Advanced RAG
‡∏û‡∏±‡∏í‡∏ô‡∏≤ RAG ‡πÉ‡∏´‡πâ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô  
- **Query Construction**:  
  - ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡πÄ‡∏õ‡πá‡∏ô SQL, Cypher, Graph-based  
  - Tools: Text-to-SQL, Knowledge Graph Integration  
- **Agents**:  
  - *Tool Selection*: Google Search, Python Interpreter  
  - *Multi-Agent Systems*: Collaborative Agents  
- **Post-Processing**:  
  - *RAG-Fusion*: ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ Retrievers  
  - *Context Compression*: ‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô  
- **Evaluation**:  
  - ‡∏ß‡∏±‡∏î Latency, Answer Quality, Cost  
- **Notebooks**:  
  - [Build Agentic RAG with LlamaIndex](https://colab.research.google.com/drive/1qW7uNR3S3h1l_9h2xS2KX9rvzX-VVvMang) - [Article](https://originshq.com/blog/build-agentic-rag-with-llamaindex/)  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LangChain - SQL with RAG](https://python.langchain.com/docs/use_cases/qa_structured/sql) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  
  - [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task) - ‡πÇ‡∏î‡∏¢ Omar Khattab  
  - [LlamaIndex - Agents](https://docs.llamaindex.ai/en/stable/examples/agent/) - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á  

---

### ‚ö° 5. Inference Optimization
‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô LLMs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥  
- **Flash Attention**:  
  - ‡∏•‡∏î Complexity ‡∏à‡∏≤‡∏Å O(n¬≤) ‡πÄ‡∏õ‡πá‡∏ô O(n)  
  - ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Transformer Inference  
- **Key-Value Cache Optimization**:  
  - *Multi-Query Attention (MQA)*: ‡∏•‡∏î KV Cache  
  - *Grouped-Query Attention (GQA)*: ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏î‡∏∏‡∏•  
- **Speculative Decoding**:  
  - Draft Model ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ ‡πÅ‡∏•‡πâ‡∏ß Refine  
- **Dynamic Batching**:  
  - ‡∏£‡∏ß‡∏° Requests ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° Throughput  
- **Hardware Acceleration**:  
  - GPU (CUDA), TPU, Apple Silicon  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Hugging Face - GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  
  - [Databricks - LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) - Best Practices  
  - [Flash Attention Paper](https://arxiv.org/abs/2205.14135) - ‡πÇ‡∏î‡∏¢ Tri Dao  
  - [Speculative Decoding](https://arxiv.org/abs/2211.17192) - Paper  

---

### üåê 6. Deploying LLMs
‡∏ô‡∏≥ LLMs ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Production  
- **Local Deployment**:  
  - *Ollama*: ‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô Docker  
  - *oobabooga/text-generation-webui*: UI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Local Models  
- **Demo Applications**:  
  - *Gradio*: ‡∏™‡∏£‡πâ‡∏≤‡∏á Web App ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ  
  - *Streamlit*: Interactive Dashboards  
- **Server Deployment**:  
  - *Text Generation Inference (TGI)*: Hugging Face Server  
  - *vLLM*: High-Throughput Inference  
  - *Ray Serve*: Scalable Serving  
- **Cloud Options**:  
  - AWS SageMaker, Google Vertex AI, Azure ML  
- **Notebooks**:  
  - [Deploy LLM with Gradio](https://colab.research.google.com/drive/1xXw0qlv-GZzovmWv2sTjMcgrKkN6gR4S) - [Article](https://originshq.com/blog/deploy-your-llm-with-gradio/)  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Streamlit - LLM App](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠  
  - [Hugging Face TGI](https://huggingface.co/docs/text-generation-inference/en/index) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£  
  - [vLLM Documentation](https://vllm.ai/) - High-Performance Serving  
  - [Ray Serve Guide](https://docs.ray.io/en/latest/serve/index.html) - Scalable Deployment  

---

### üîí 7. Securing LLMs
‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á LLMs ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°  
- **Prompt Hacking**:  
  - *Prompt Injection*: ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢  
  - *Jailbreaking*: ‡∏ö‡∏≤‡∏¢‡∏û‡∏≤‡∏™‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î  
- **Defensive Measures**:  
  - *Input Sanitization*: ‡∏Å‡∏£‡∏≠‡∏á Prompt  
  - *Red Teaming*: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô  
  - *Guardrails*: ‡∏à‡∏≥‡∏Å‡∏±‡∏î Output  
- **Monitoring**:  
  - Log Usage, Detect Anomalies  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á  
  - [LLM Security](https://llmsecurity.net/) - ‡πÇ‡∏î‡∏¢ Lakera  
  - [Prompt Injection Guide](https://simonwillison.net/2023/Oct/31/prompt-injection-explained/) - ‡πÇ‡∏î‡∏¢ Simon Willison  
  - [Guardrails AI](https://github.com/ShreyaR/guardrails) - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î  

---

### üìù Advanced Scripts and Repositories
- **LangChain**:  
  - [GitHub](https://github.com/langchain-ai/langchain) - RAG ‡πÅ‡∏•‡∏∞ Agents  
- **LlamaIndex**:  
  - [GitHub](https://github.com/run-llama/llama_index) - Data Ingestion ‡πÅ‡∏•‡∏∞ Query  
- **Unsloth**:  
  - [Qwen 2 VL (7B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) - Multimodal  
- **Awesome Colab**:  
  - [Text Generation WebUI](https://colab.research.google.com/github/oobabooga/text-generation-webui/blob/main/notebooks/colab.ipynb) - Local Deployment  

---

### üöÄ How to Proceed
1. **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô**: ‡∏£‡∏±‡∏ô LLM ‡∏î‡πâ‡∏ß‡∏¢ API ‡∏´‡∏£‡∏∑‡∏≠ Local  
2. **‡∏™‡∏£‡πâ‡∏≤‡∏á RAG**: ‡πÉ‡∏ä‡πâ LangChain/LlamaIndex  
3. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á**: ‡πÄ‡∏û‡∏¥‡πà‡∏° Agents ‡∏´‡∏£‡∏∑‡∏≠ Optimize Inference  
4. **Deploy**: ‡∏•‡∏≠‡∏á Gradio ‡∏´‡∏£‡∏∑‡∏≠ TGI  

# Awesome AI/LLM Learning Resources for 2025 (Part 4/4)

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*  
*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

## Part 4: GitHub Repositories and Advanced Scripts

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö GitHub ‡πÅ‡∏•‡∏∞ Scripts ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô AI, Machine Learning (ML), Deep Learning (DL) ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á Fine-Tuning, Distributed Training ‡πÅ‡∏•‡∏∞ Advanced Applications ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

---

### üìÇ GitHub Repositories

#### 1. Unsloth Notebooks  
- **GitHub**: [unslothai/notebooks](https://github.com/unslothai/notebooks)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÅ‡∏•‡∏∞ Inference LLMs ‡∏ö‡∏ô Google Colab ‡πÅ‡∏•‡∏∞ Kaggle  
- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
  - **GRPO Notebooks**:  
    - [Phi 4 (14B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb) - Fine-Tuning ‡∏î‡πâ‡∏ß‡∏¢ GRPO  
    - [Llama 3.1 (8B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)  
  - **Llama Notebooks**:  
    - [Llama 3.2 (1B and 3B) - Conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)  
    - [Llama 3.2 (11B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb) - Multimodal  
  - **Mistral Notebooks**:  
    - [Mistral Small (22B) - Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb)  
    - [Mistral (7B) - Text Completion](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)  
  - **Multimodal**:  
    - [Qwen 2 VL (7B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)  
- **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå**: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Colab ‡πÅ‡∏•‡∏∞ Kaggle ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏á‡πà‡∏≤‡∏¢  

#### 2. Awesome Colab Notebooks  
- **GitHub**: [amrzv/awesome-colab-notebooks](https://github.com/amrzv/awesome-colab-notebooks)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML Experiments ‡πÅ‡∏•‡∏∞ Research  
- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
  - **Courses**:  
    - [ARENA](https://colab.research.google.com/drive/1vuQOB2Gd7OcfzH2y9djXm9OdZA_DcxYz) - ML Engineering ‡πÇ‡∏î‡∏¢ Callum McDougall  
    - [Autodiff Cookbook](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb) - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô JAX  
  - **Research**:  
    - [AlphaFold](https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb) - Protein Structure Prediction  
    - [DeepLabCut](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_maDLC_TrainNetwork_VideoAnalysis.ipynb) - Motion Tracking  
  - **Applications**:  
    - [Text Generation WebUI](https://colab.research.google.com/github/oobabooga/text-generation-webui/blob/main/notebooks/colab.ipynb) - Deploy Local LLMs  
    - [ModernBERT](https://colab.research.google.com/github/AnswerDotAI/ModernBERT/blob/master/examples/finetune_modernbert_on_glue.ipynb) - Fine-Tuning BERT  
- **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå**: ‡∏£‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢  

#### 3. AI-ML-DL Projects  
- **GitHub**: [theakash07/AI-ML-DL-Projects](https://github.com/theakash07/AI-ML-DL-Projects)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: 40+ ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢  
- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
  - [365 Days Computer Vision](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/365-Days-Computer-Vision-Learning) - ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå CV ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô  
  - [125+ NLP Language Models](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/125-NLP-Language-Models) - ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• NLP  
  - [Generative AI](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/Generative-AI) - GANs, Diffusion Models  
- **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå**: ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Portfolio  

#### 4. ml-systems-papers  
- **GitHub**: [byungsoo-oh/ml-systems-papers](https://github.com/byungsoo-oh/ml-systems-papers)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Paper ‡πÅ‡∏•‡∏∞ Scripts ‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥ (SOSP, NeurIPS, SC)  
- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
  - [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor) - Nested Data Parallelism  
  - [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain) - Long-Sequence Training  
  - [TorchTitan](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_TorchTitan) - PyTorch Distributed Training  
  - [DeepSpeed-Ulysses](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_DeepSpeed-Ulysses) - Long-Sequence Transformers  
- **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå**: ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á  

#### 5. Additional Repositories  
- **Hugging Face Transformers**: [GitHub](https://github.com/huggingface/transformers) - Library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NLP ‡πÅ‡∏•‡∏∞ LLMs  
- **LangChain**: [GitHub](https://github.com/langchain-ai/langchain) - RAG ‡πÅ‡∏•‡∏∞ Agents  
- **LlamaIndex**: [GitHub](https://github.com/run-llama/llama_index) - Data Ingestion ‡πÅ‡∏•‡∏∞ Query  

---

### ‚öôÔ∏è Advanced Scripts

#### Fine-Tuning & Optimization  
1. **[FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor)**  
   - Nested Data Parallelism ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Training LLMs  
   - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)  
2. **[4D Parallelism](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_4D_Parallelism)**  
   - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß Training ‡∏î‡πâ‡∏ß‡∏¢ 4D Parallelism  
   - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)  
3. **[QLoRA Fine-Tuning](https://github.com/georgesung/llm_qlora/blob/main/train.py)**  
   - Fine-Tuning LLMs ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏î‡πâ‡∏ß‡∏¢ 4-bit Quantization  
   - Paper: [QLoRA](https://arxiv.org/abs/2305.14314)  
4. **[LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain)**  
   - Fine-Tuning Long-Sequence LLMs  
   - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)  

#### Distributed Training  
5. **[Democratizing AI](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SC24_GPU_Supercomputers)**  
   - ‡∏ù‡∏∂‡∏Å LLMs ‡∏ö‡∏ô GPU Supercomputers  
   - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)  
6. **[TorchTitan](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_TorchTitan)**  
   - PyTorch Native Solution ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Distributed Training  
   - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)  
7. **[DistTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_DistTrain)**  
   - Disaggregated Training ‡∏ö‡∏ô Hardware ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß  
   - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)  

#### Advanced Applications  
8. **[DeepSpeed-Ulysses](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_DeepSpeed-Ulysses)**  
   - Long-Sequence Transformers ‡∏î‡πâ‡∏ß‡∏¢ DeepSpeed  
   - Paper: [arXiv:2309.14525](https://arxiv.org/abs/2309.14525)  
9. **[FLM-101B](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_FLM-101B)**  
   - Fine-Tuning ‡πÇ‡∏°‡πÄ‡∏î‡∏• 101B Parameters ‡∏î‡πâ‡∏ß‡∏¢‡∏á‡∏ö $100K  
   - Paper: [arXiv:2309.14525](https://arxiv.org/abs/2309.14525)  
10. **[LongLoRA](https://github.com/dvlab-research/LongLoRA)**  
    - Fine-Tuning Long-Context LLMs  
    - Paper: [arXiv:2309.12307](https://arxiv.org/abs/2309.12307)  

---

### üìú Research Papers
- **Fine-Tuning**:  
  - [QLoRA](https://arxiv.org/abs/2305.14314) - Quantized LoRA  
  - [LongLoRA](https://arxiv.org/abs/2309.12307) - Long-Context Fine-Tuning  
  - [LoRA](https://arxiv.org/abs/2106.09685) - Low-Rank Adaptation  
- **Distributed Training**:  
  - [Democratizing AI](https://arxiv.org/abs/2409.12345) - GPU Supercomputers  
  - [TorchTitan](https://arxiv.org/abs/2409.12345) - PyTorch Solution  
  - [DeepSpeed-Ulysses](https://arxiv.org/abs/2309.14525) - Long-Sequence Optimization  
- **Advanced Techniques**:  
  - [Flash Attention](https://arxiv.org/abs/2205.14135) - Optimized Attention  
  - [Speculative Decoding](https://arxiv.org/abs/2211.17192) - Faster Inference  

---

### üöÄ How to Use
1. **‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Repository**:  
   - ‡∏Ñ‡∏•‡∏¥‡∏Å "Code" > "Download ZIP" ‡∏ö‡∏ô GitHub ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ `git clone <URL>`  
2. **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies**:  
   - ‡∏£‡∏±‡∏ô `pip install -r requirements.txt` ‡πÉ‡∏ô Terminal  
3. **‡∏£‡∏±‡∏ô Scripts**:  
   - ‡πÉ‡∏ä‡πâ `python script_name.py` ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Notebook ‡πÉ‡∏ô Colab/Kaggle  
4. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á**:  
   - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô Dataset, Model Size)  
5. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö**:  
   - ‡∏£‡∏±‡∏ô‡∏ö‡∏ô Hardware ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (GPU ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏Å)  


# Awesome AI/LLM Learning Resources for 2025 (Part 5/5)


