#  AI/LLM Learning Resources 2025 by zombitx64

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*

*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

‡∏Ñ‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏î‡πâ‡∏≤‡∏ô Artificial Intelligence (AI) ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1.  [‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô](#‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
    *   [‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning](#‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö-machine-learning)
    *   [Python ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI](#python-‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö-ai)
    *   [Neural Networks](#neural-networks)
    *   [Natural Language Processing (NLP)](#natural-language-processing-nlp)
2.  [‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô](#‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)
    *   [Notebooks ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥](#notebooks-‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
    *   [‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô](#‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô)
3.  [‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£](#‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£)
    *   [Datasets](#datasets)
    *   [Tools](#tools)
    *   [‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°](#‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°)

## ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô <a name="‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"></a>

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

---

### üìö LLM Fundamentals

#### 1. ‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning <a name="‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö-machine-learning"></a>

‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á AI ‡πÅ‡∏•‡∏∞ Machine Learning (ML) ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•

*   **Linear Algebra:**
    *   ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° Vectors, Matrices, Derivatives, Integrals, Limits, Series, Multivariable Calculus ‡πÅ‡∏•‡∏∞ Gradient Concepts
    *   ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deep Learning ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏ô Neural Networks
*   **Probability and Statistics:**
    *   ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    *   ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô Probability Distributions, Hypothesis Testing, Bayesian Inference
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [Mathematics for Machine Learning](https://mml-book.github.io/) - ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ü‡∏£‡∏µ‡∏à‡∏≤‡∏Å Cambridge University Press
    *   [3Blue1Brown - Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) - ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏™‡∏≠‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
    *   [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability) - ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏ü‡∏£‡∏µ

#### 2. Python ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI <a name="python-‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö-ai"></a>

Python ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡πÅ‡∏•‡∏∞ LLMs ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ Libraries ‡∏ó‡∏µ‡πà‡∏ó‡∏£‡∏á‡∏û‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢

*   **‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Python:** Variables, Functions, Loops, Data Structures (Lists, Dictionaries)
*   **Libraries ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
    *   *NumPy:* ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    *   *Pandas:* ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    *   *Matplotlib:* ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÇ‡∏î‡∏¢ Jake VanderPlas ‡∏û‡∏£‡πâ‡∏≠‡∏° Notebooks
    *   [RealPython](https://realpython.com/) - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô Python ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î
    *   [CS231n Python Tutorial](https://cs231n.github.io/python-numpy-tutorial/) - ‡∏à‡∏≤‡∏Å Stanford

#### 3. Neural Networks <a name="neural-networks"></a>

Neural Networks ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á Deep Learning ‡πÅ‡∏•‡∏∞ LLMs ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡πÑ‡∏õ‡∏™‡∏π‡πà Transformer Models

*   **‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:**
    *   Layers, Weights, Biases, Activation Functions (Sigmoid, Tanh, ReLU)
    *   ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Feedforward ‡πÅ‡∏•‡∏∞ Backpropagation
*   **‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•:**
    *   Loss Functions, Optimization (Gradient Descent), Overfitting Prevention
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ü‡∏£‡∏µ‡πÇ‡∏î‡∏¢ Michael Nielsen
    *   [CS231n: Convolutional Neural Networks](https://cs231n.github.io/) - ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏à‡∏≤‡∏Å Stanford
    *   [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) - ‡πÇ‡∏î‡∏¢ Andrew NG ‡∏ö‡∏ô Coursera

#### 4. Natural Language Processing (NLP) <a name="natural-language-processing-nlp"></a>

NLP ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£ ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á LLMs

*   **Text Preprocessing:**
    *   *Tokenization:* ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
    *   *Stemming/Lemmatization:* ‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏£‡∏≤‡∏Å
    *   *Stop Word Removal:* ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
*   **Feature Extraction Techniques:**
    *   *Bag-of-Words (BoW):* ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ñ‡∏≥
    *   *TF-IDF:* ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    *   *N-grams:* ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
*   **Word Embeddings:**
    *   *Word2Vec, GloVe, FastText:* ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô
*   **Recurrent Neural Networks (RNNs):**
    *   ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏≥‡∏î‡∏±‡∏ö (Sequential Data)
    *   Variants ‡πÄ‡∏ä‡πà‡∏ô LSTMs ‡πÅ‡∏•‡∏∞ GRUs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö Long-Term Dependencies
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html) - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Word Embeddings
    *   [RealPython - NLP with spaCy](https://realpython.com/natural-language-processing-spacy-python/) - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ spaCy
    *   [Jay Alammar - Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - ‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
    *   [Colah‚Äôs Blog - Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ LSTMs
    *   [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Kaggle

---

# Awesome AI/LLM Learning Resources for 2025 (Part 2/4)

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*

*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

## Part 2: The LLM Scientist - Advanced LLM Development

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á LLMs ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°, ‡∏Å‡∏≤‡∏£ Pre-training, Post-training, Fine-Tuning, Preference Alignment, ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•, Quantization ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡πÉ‡∏´‡∏°‡πà ‡πÜ

---

### üß† 1. LLM Architecture

‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á LLMs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•

*   **Key Topics:**
    *   **Architectural Overview:** Evolution ‡∏à‡∏≤‡∏Å *Encoder-Decoder Transformers* (‡πÄ‡∏ä‡πà‡∏ô BERT) ‡∏™‡∏π‡πà *Decoder-Only* (‡πÄ‡∏ä‡πà‡∏ô GPT) ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á
    *   **Tokenization:** ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Numerical Tokens) ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ ‡πÄ‡∏ä‡πà‡∏ô Byte-Pair Encoding (BPE), WordPiece
    *   **Attention Mechanisms:** *Self-Attention*: ‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Variants ‡πÄ‡∏ä‡πà‡∏ô Multi-Head Attention, Long-Range Dependencies
    *   **Sampling Techniques:** *Deterministic*: Greedy Search, Beam Search *Probabilistic*: Temperature Sampling, Nucleus Sampling
*   **Resources:**
    *   [3Blue1Brown - Visual Intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Transformer ‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏û
    *   [Andrej Karpathy - nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) - ‡∏™‡∏£‡πâ‡∏≤‡∏á GPT ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å (‡∏°‡∏µ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ Tokenization: [Link](https://www.youtube.com/watch?v=zduSFxRajkE))
    *   [Lilian Weng - Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) - ‡∏Å‡∏•‡πÑ‡∏Å Attention
    *   [Maxime Labonne - Decoding Strategies](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

---

### ‚öôÔ∏è 2. Pre-training Models

Pre-training ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏π‡∏á ‡πÅ‡∏ï‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

*   **Key Topics:**
    *   **Data Preparation:** ‡πÉ‡∏ä‡πâ Dataset ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (‡πÄ‡∏ä‡πà‡∏ô Llama 3.1 ‡∏ù‡∏∂‡∏Å‡∏ö‡∏ô 15T Tokens) ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô: Curate, Clean, Deduplicate, Tokenize, Quality Filtering
    *   **Distributed Training:** *Data Parallelism*: ‡πÅ‡∏ö‡πà‡∏á Batch ‡πÑ‡∏õ‡∏¢‡∏±‡∏á GPU ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß *Pipeline Parallelism*: ‡πÅ‡∏ö‡πà‡∏á Layers *Tensor Parallelism*: ‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
    *   **Training Optimization:** Adaptive Learning Rates, Gradient Clipping, Mixed-Precision Training Optimizers: AdamW, Lion
    *   **Monitoring:** ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° Loss, Gradients, GPU Usage ‡∏î‡πâ‡∏ß‡∏¢ Dashboards
*   **Resources:**
    *   [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) - Dataset ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÇ‡∏î‡∏¢ Penedo et al.
    *   [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2) - Dataset ‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏î‡∏¢ Weber et al.
    *   [Nanotron](https://github.com/huggingface/nanotron) - ‡πÉ‡∏ä‡πâ‡∏ù‡∏∂‡∏Å SmolLM2 ([Link](https://github.com/huggingface/smollm))
    *   [Parallel Training](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf) - ‡πÇ‡∏î‡∏¢ Chenyan Xiong
    *   [Distributed Training](https://arxiv.org/abs/2407.20018) - Paper ‡πÇ‡∏î‡∏¢ Duan et al.
    *   [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor) - Nested Data Parallelism

---

### üìä 3. Post-training Datasets

Post-training ‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

*   **Key Topics:**
    *   **Storage & Chat Templates:** Formats: ShareGPT, OpenAI/HF Chat Templates: ChatML, Alpaca
    *   **Synthetic Data Generation:** ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á (‡πÄ‡∏ä‡πà‡∏ô GPT-4o) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏π‡πà Instruction-Response ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: Diverse Seed Tasks, Effective Prompts
    *   **Data Enhancement:** Verified Outputs (Unit Tests), Rejection Sampling, Auto-Evol ([Paper](https://arxiv.org/abs/2406.00770)) Chain-of-Thought, Branch-Solve-Merge, Persona-based
    *   **Quality Filtering:** Rule-based, Duplicate Removal (MinHash/Embeddings), N-gram Decontamination ‡πÉ‡∏ä‡πâ Reward Models ‡πÅ‡∏•‡∏∞ Judge LLMs
*   **Resources:**
    *   [LLM Datasets](https://github.com/mlabonne/llm-datasets) - ‡∏Ñ‡∏•‡∏±‡∏á Dataset ‡πÇ‡∏î‡∏¢ Maxime Labonne
    *   [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator) - ‡πÇ‡∏î‡∏¢ Argilla
    *   [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator) - ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Dataset
    *   [Distilabel](https://distilabel.argilla.io/dev/sections/pipeline_samples/) - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
    *   [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Hugging Face

---

### üîß 4. Supervised Fine-Tuning (SFT)

SFT ‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ

*   **Key Topics:**
    *   **Training Techniques:** *Full Fine-Tuning*: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏∏‡∏Å‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏π‡∏á) *LoRA*: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Adapter Parameters ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ *QLoRA*: ‡∏£‡∏ß‡∏° 4-bit Quantization ‡∏Å‡∏±‡∏ö LoRA
    *   **Training Parameters:** Learning Rate (‡∏Å‡∏±‡∏ö Schedulers), Batch Size, Gradient Accumulation Optimizers: 8-bit AdamW, Weight Decay, Warmup Steps LoRA Parameters: Rank, Alpha, Target Modules
    *   **Distributed Training:** DeepSpeed (ZeRO Optimization), FSDP, Gradient Checkpointing
    *   **Monitoring:** Loss Curves, Learning Rate Changes, Gradient Norms
*   **Notebooks:**
    *   [Fine-tune Llama 3.1 with Unsloth](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z) - [Article](https://originshq.com/blog/fine-tune-llama-3-1-ultra-efficiently-with-unsloth/)
    *   [Fine-tune Mistral-7b with QLoRA](https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS) - ‡πÉ‡∏ä‡πâ TRL
    *   [Llama 3.1 (8B) - Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)
*   **Resources:**
    *   [Fine-tune Llama 3.1 with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) - ‡πÇ‡∏î‡∏¢ Maxime Labonne
    *   [Axolotl Documentation](https://axolotl-ai-cloud.github.io/axolotl/) - ‡πÇ‡∏î‡∏¢ Wing Lian
    *   [LoRA Insights](https://lightning.ai/pages/community/lora-insights/) - ‡πÇ‡∏î‡∏¢ Sebastian Raschka
    *   [QLoRA Fine-Tuning](https://github.com/georgesung/llm_qlora/blob/main/train.py) - Script

---

### üéØ 5. Preference Alignment

‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô Toxicity, Hallucinations

*   **Key Topics:**
    *   **Rejection Sampling:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡πà‡∏≠ Prompt ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å/‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò
    *   **Direct Preference Optimization (DPO):** ‡πÄ‡∏û‡∏¥‡πà‡∏° Likelihood ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ([Paper](https://arxiv.org/abs/2305.18290))
    *   **Proximal Policy Optimization (PPO):** ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Policy ‡∏î‡πâ‡∏ß‡∏¢ Reward Model ([Paper](https://arxiv.org/abs/1707.06347))
    *   **Monitoring:** Margin ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Chosen/Rejected Responses, Accuracy
*   **Notebooks:**
    *   [Fine-tune Mistral-7b with DPO](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE) - [Article](https://originshq.com/blog/boost-the-performance-of-supervised-fine-tuned-models-with-dpo/)
    *   [Fine-tune Llama 3 with ORPO](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi) - [Article](https://originshq.com/blog/fine-tune-llama-3-with-orpo/)
*   **Resources:**
    *   [Illustrating RLHF](https://huggingface.co/blog/rlhf) - ‡πÇ‡∏î‡∏¢ Hugging Face
    *   [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
    *   [DPO Wandb Logs](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4) - ‡πÇ‡∏î‡∏¢ Alexander Vishnevskiy

---

### üìà 6. Evaluation

‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• LLMs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Dataset ‡πÅ‡∏•‡∏∞ Training

*   **Key Topics:**
    *   **Automated Benchmarks:** ‡πÉ‡∏ä‡πâ MMLU, TriviaQA ‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
    *   **Human Evaluation:** Community Voting (‡πÄ‡∏ä‡πà‡∏ô Arena), Subjective Assessments
    *   **Model-based Evaluation:** Judge Models, Reward Models
    *   **Feedback Signal:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Error Patterns ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
*   **Resources:**
    *   [Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook) - ‡πÇ‡∏î‡∏¢ Cl√©mentine Fourrier
    *   [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) - ‡πÇ‡∏î‡∏¢ Hugging Face
    *   [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) - ‡πÇ‡∏î‡∏¢ EleutherAI
    *   [Chatbot Arena](https://lmarena.ai/) - ‡πÇ‡∏î‡∏¢ LMSYS

---

### ‚ö° 7. Quantization

‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡∏ô Hardware ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

*   **Key Topics:**
    *   **Base Techniques:** Precisions: FP32, FP16, INT8, 4-bit Methods: Absmax, Zero-point
    *   **GGUF & llama.cpp:** ‡∏£‡∏±‡∏ô‡∏ö‡∏ô CPU/GPU ‡∏î‡πâ‡∏ß‡∏¢ [llama.cpp](https://github.com/ggerganov/llama.cpp)
    *   **GPTQ & AWQ:** Layer-wise Calibration ([GPTQ Paper](https://arxiv.org/abs/2210.17323), [AWQ Paper](https://arxiv.org/abs/2306.00978))
    *   **SmoothQuant & ZeroQuant:** ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô Quantization
*   **Notebooks:**
    *   [4-bit Quantization with GPTQ](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A) - [Article](https://originshq.com/blog/4-bit-llm-quantization-with-gptq/)
    *   [Quantization with GGUF](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD) - [Article](https://originshq.com/blog/quantize-llama-models-with-gguf-and-llama-cpp/)
*   **Resources:**
    *   [Introduction to Quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) - ‡πÇ‡∏î‡∏¢ Maxime Labonne
    *   [DeepSpeed Model Compression](https://www.deepspeed.ai/tutorials/model-compression/) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠

---

### üåü 8. New Trends

‡∏™‡∏≥‡∏£‡∏ß‡∏à‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÉ‡∏ô‡∏ß‡∏á‡∏Å‡∏≤‡∏£ LLMs

*   **Key Topics:**
    *   **Model Merging:** ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ [Mergekit](https://github.com/cg123/mergekit) (SLERP, DARE, TIES)
    *   **Multimodal Models:** CLIP, LLaVA, Stable Diffusion - ‡∏£‡∏ß‡∏° Text, Image, Audio
    *   **Interpretability:** Sparse Autoencoders (SAEs), Abliteration - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•
    *   **Test-time Compute:** ‡∏õ‡∏£‡∏±‡∏ö Compute ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Inference (‡πÄ‡∏ä‡πà‡∏ô Process Reward Model)
*   **Notebooks:**
    *   [Merge LLMs with Mergekit](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr) - [Article](https://originshq.com/blog/merge-large-language-models-with-mergekit/)
    *   [Uncensor LLM with Abliteration](https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR) - [Article](https://originshq.com/blog/uncensor-any-llm-with-abliteration/)
*   **Resources:**
    *   [Merge LLMs with Mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html) - ‡πÇ‡∏î‡∏¢ Maxime Labonne
    *   [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) - ‡πÇ‡∏î‡∏¢ Chip Huyen
    *   [Intuitive Explanation of SAEs](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) - ‡πÇ‡∏î‡∏¢ Adam Karvonen
    *   [Scaling Test-time Compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) - ‡πÇ‡∏î‡∏¢ Beeching et al.

---

### üìù Advanced Scripts and Repositories

*   **Unsloth:**
    *   [GitHub](https://github.com/unslothai/unsloth) - Tools ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÅ‡∏•‡∏∞ Quantization
*   **ml-systems-papers:**
    *   [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor) - Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)
    *   [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain) - Long-Sequence Training
*   **Awesome Colab:**
    *   [ModernBERT](https://colab.research.google.com/github/AnswerDotAI/ModernBERT/blob/master/examples/finetune_modernbert_on_glue.ipynb) - Fine-Tuning Encoder Models

---

### üöÄ How to Proceed

1.  **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Architecture ‡∏´‡∏£‡∏∑‡∏≠ Pre-training
2.  **‡∏ó‡∏î‡∏•‡∏≠‡∏á Notebooks:** ‡∏£‡∏±‡∏ô‡πÉ‡∏ô Colab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥
3.  **‡∏®‡∏∂‡∏Å‡∏©‡∏≤ Papers:** ‡∏≠‡πà‡∏≤‡∏ô Paper ‡πÅ‡∏•‡∏∞ Scripts ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å


# Awesome AI/LLM Learning Resources for 2025 (Part 3/4)

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*
*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

## Part 3: The LLM Engineer - Building LLM Applications

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥ LLMs ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•, ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Storage, Retrieval Augmented Generation (RAG), ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á RAG ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á, ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Inference, ‡∏Å‡∏≤‡∏£ Deploy ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•

---

### üöÄ 1. Running LLMs

‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ô LLMs ‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Prompt

*   **Using APIs:**
    *   ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö OpenAI, Hugging Face Inference API, Grok API
    *   **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢, Scalable
*   **Local Deployment:**
    *   **Tools:** LM Studio, Ollama, llama.cpp
    *   **Hardware:** CPU, GPU, Mac M1/M2
*   **Prompt Engineering:**
    *   ***Zero-Shot:*** ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    *   ***Few-Shot:*** ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô Prompt
    *   ***Prompt Chaining:*** ‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [Prompt Engineering Guide](https://www.promptingguide.ai/) - ‡πÇ‡∏î‡∏¢ DAIR.AI
    *   [Run LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
    *   [Hugging Face Inference API](https://huggingface.co/docs/api-inference/quicktour) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    *   [Ollama Documentation](https://ollama.ai/docs) - ‡∏£‡∏±‡∏ô Local Models

---

### üìÇ 2. Building Vector Storage

‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ LLMs ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å

*   **Document Ingestion:**
    *   ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå: PDF, JSON, CSV, Markdown
    *   **Tools:** PyPDF2, pdfplumber
*   **Text Splitting:**
    *   *Recursive Splitting:* ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£, Tokens
    *   *Semantic Splitting:* ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
*   **Embedding Models:**
    *   *Sentence Transformers:* All-MiniLM-L6-v2, BGE
    *   *OpenAI Embeddings:* text-embedding-ada-002
*   **Vector Databases:**
    *   *Chroma:* Open-source, Local
    *   *Pinecone:* Cloud-based, Scalable
    *   *FAISS:* High-performance Similarity Search
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [LangChain - Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
    *   [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Embedding Model Rankings
    *   [Pinecone - Vector Search](https://www.pinecone.io/learn/vector-search/) - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
    *   [Chroma Docs](https://docs.trychroma.com/) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

---

### üîç 3. Retrieval Augmented Generation (RAG)

‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Retrieval) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Generation)

*   **Orchestrators:**
    *   *LangChain:* ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Workflow, Memory
    *   *LlamaIndex:* Data Ingestion, Query Engine
*   **Retrievers:**
    *   *Multi-Query:* ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
    *   *HyDE:* Hypothetical Document Embeddings
    *   *Parent Document Retrieval:* ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
*   **Evaluation:**
    *   *Ragas:* ‡∏ß‡∏±‡∏î Faithfulness, Relevance
    *   *DeepEval:* Metrics ‡πÄ‡∏ä‡πà‡∏ô BLEU, ROUGE
*   **Notebooks:**
    *   [LangChain RAG](https://colab.research.google.com/drive/1f3VFD6jCSvK0uo2Q84-TT1AbfunN-UEZ) - [Article](https://originshq.com/blog/build-a-retrieval-augmented-generation-rag-app-with-langchain/)
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [LangChain - Q&A with RAG](https://python.langchain.com/docs/use_cases/question_answering/quickstart) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
    *   [LlamaIndex Docs](https://docs.llamaindex.ai/en/stable/) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    *   [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/) - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
    *   [Ragas Documentation](https://docs.ragas.io/en/stable/) - Evaluation Framework

---

### ‚öôÔ∏è 4. Advanced RAG

‡∏û‡∏±‡∏í‡∏ô‡∏≤ RAG ‡πÉ‡∏´‡πâ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô

*   **Query Construction:**
    *   ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡πÄ‡∏õ‡πá‡∏ô SQL, Cypher, Graph-based
    *   **Tools:** Text-to-SQL, Knowledge Graph Integration
*   **Agents:**
    *   *Tool Selection:* Google Search, Python Interpreter
    *   *Multi-Agent Systems:* Collaborative Agents
*   **Post-Processing:**
    *   *RAG-Fusion:* ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ Retrievers
    *   *Context Compression:* ‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
*   **Evaluation:**
    *   ‡∏ß‡∏±‡∏î Latency, Answer Quality, Cost
*   **Notebooks:**
    *   [Build Agentic RAG with LlamaIndex](https://colab.research.google.com/drive/1qW7uNR3S3h1l_9h2xS2KX9rvzX-VVvMang) - [Article](https://originshq.com/blog/build-agentic-rag-with-llamaindex/)
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [LangChain - SQL with RAG](https://python.langchain.com/docs/use_cases/qa_structured/sql) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
    *   [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task) - ‡πÇ‡∏î‡∏¢ Omar Khattab
    *   [LlamaIndex - Agents](https://docs.llamaindex.ai/en/stable/examples/agent/) - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á

---

### ‚ö° 5. Inference Optimization

‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô LLMs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥

*   **Flash Attention:**
    *   ‡∏•‡∏î Complexity ‡∏à‡∏≤‡∏Å O(n¬≤) ‡πÄ‡∏õ‡πá‡∏ô O(n)
    *   ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Transformer Inference
*   **Key-Value Cache Optimization:**
    *   *Multi-Query Attention (MQA):* ‡∏•‡∏î KV Cache
    *   *Grouped-Query Attention (GQA):* ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏î‡∏∏‡∏•
*   **Speculative Decoding:**
    *   Draft Model ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ ‡πÅ‡∏•‡πâ‡∏ß Refine
*   **Dynamic Batching:**
    *   ‡∏£‡∏ß‡∏° Requests ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° Throughput
*   **Hardware Acceleration:**
    *   GPU (CUDA), TPU, Apple Silicon
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [Hugging Face - GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
    *   [Databricks - LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) - Best Practices
    *   [Flash Attention Paper](https://arxiv.org/abs/2205.14135) - ‡πÇ‡∏î‡∏¢ Tri Dao
    *   [Speculative Decoding](https://arxiv.org/abs/2211.17192) - Paper

---

### üåê 6. Deploying LLMs

‡∏ô‡∏≥ LLMs ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Production

*   **Local Deployment:**
    *   *Ollama:* ‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô Docker
    *   *oobabooga/text-generation-webui:* UI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Local Models
*   **Demo Applications:**
    *   *Gradio:* ‡∏™‡∏£‡πâ‡∏≤‡∏á Web App ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
    *   *Streamlit:* Interactive Dashboards
*   **Server Deployment:**
    *   *Text Generation Inference (TGI):* Hugging Face Server
    *   *vLLM:* High-Throughput Inference
    *   *Ray Serve:* Scalable Serving
*   **Cloud Options:**
    *   AWS SageMaker, Google Vertex AI, Azure ML
*   **Notebooks:**
    *   [Deploy LLM with Gradio](https://colab.research.google.com/drive/1xXw0qlv-GZzovmWv2sTjMcgrKkN6gR4S) - [Article](https://originshq.com/blog/deploy-your-llm-with-gradio/)
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [Streamlit - LLM App](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
    *   [Hugging Face TGI](https://huggingface.co/docs/text-generation-inference/en/index) - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    *   [vLLM Documentation](https://vllm.ai/) - High-Performance Serving
    *   [Ray Serve Guide](https://docs.ray.io/en/latest/serve/index.html) - Scalable Deployment

---

### üîí 7. Securing LLMs

‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á LLMs ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

*   **Prompt Hacking:**
    *   *Prompt Injection:* ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢
    *   *Jailbreaking:* ‡∏ö‡∏≤‡∏¢‡∏û‡∏≤‡∏™‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î
*   **Defensive Measures:**
    *   *Input Sanitization:* ‡∏Å‡∏£‡∏≠‡∏á Prompt
    *   *Red Teaming:* ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô
    *   *Guardrails:* ‡∏à‡∏≥‡∏Å‡∏±‡∏î Output
*   **Monitoring:**
    *   Log Usage, Detect Anomalies
*   **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
    *   [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á
    *   [LLM Security](https://llmsecurity.net/) - ‡πÇ‡∏î‡∏¢ Lakera
    *   [Prompt Injection Guide](https://simonwillison.net/2023/Oct/31/prompt-injection-explained/) - ‡πÇ‡∏î‡∏¢ Simon Willison
    *   [Guardrails AI](https://github.com/ShreyaR/guardrails) - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î

---

### üìù Advanced Scripts and Repositories

*   **LangChain:**
    *   [GitHub](https://github.com/langchain-ai/langchain) - RAG ‡πÅ‡∏•‡∏∞ Agents
*   **LlamaIndex:**
    *   [GitHub](https://github.com/run-llama/llama_index) - Data Ingestion ‡πÅ‡∏•‡∏∞ Query
* **Unsloth**:
    * [Qwen 2 VL (7B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) - Multimodal

*   **Awesome Colab:**
    *   [Text Generation WebUI](https://colab.research.google.com/github/oobabooga/text-generation-webui/blob/main/notebooks/colab.ipynb) - Local Deployment

---

### üöÄ How to Proceed

1.  **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô:** ‡∏£‡∏±‡∏ô LLM ‡∏î‡πâ‡∏ß‡∏¢ API ‡∏´‡∏£‡∏∑‡∏≠ Local
2.  **‡∏™‡∏£‡πâ‡∏≤‡∏á RAG:** ‡πÉ‡∏ä‡πâ LangChain/LlamaIndex
3.  **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á:** ‡πÄ‡∏û‡∏¥‡πà‡∏° Agents ‡∏´‡∏£‡∏∑‡∏≠ Optimize Inference
4.  **Deploy:** ‡∏•‡∏≠‡∏á Gradio ‡∏´‡∏£‡∏∑‡∏≠ TGI


# Awesome AI/LLM Learning Resources for 2025 (Part 4/4)

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*
*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

## Part 4: GitHub Repositories and Advanced Scripts

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö GitHub ‡πÅ‡∏•‡∏∞ Scripts ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô AI, Machine Learning (ML), Deep Learning (DL) ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á Fine-Tuning, Distributed Training ‡πÅ‡∏•‡∏∞ Advanced Applications ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

---

### üìÇ GitHub Repositories

#### 1. Unsloth Notebooks

*   **GitHub:** [unslothai/notebooks](https://github.com/unslothai/notebooks)
*   **Description:** ‡∏Ñ‡∏•‡∏±‡∏á Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÅ‡∏•‡∏∞ Inference LLMs ‡∏ö‡∏ô Google Colab ‡πÅ‡∏•‡∏∞ Kaggle
*   **Examples:**
    *   **GRPO Notebooks:**
        *   [Phi 4 (14B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb) - Fine-Tuning ‡∏î‡πâ‡∏ß‡∏¢ GRPO
        *   [Llama 3.1 (8B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
    *   **Llama Notebooks:**
        *   [Llama 3.2 (1B and 3B) - Conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)
        *   [Llama 3.2 (11B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb) - Multimodal
    *   **Mistral Notebooks:**
        *   [Mistral Small (22B) - Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb)
        *   [Mistral (7B) - Text Completion](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)
    *   **Multimodal:**
        *   [Qwen 2 VL (7B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)
*   **Benefits:** ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Colab ‡πÅ‡∏•‡∏∞ Kaggle ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏á‡πà‡∏≤‡∏¢

#### 2. Awesome Colab Notebooks

*   **GitHub:** [amrzv/awesome-colab-notebooks](https://github.com/amrzv/awesome-colab-notebooks)
*   **Description:** ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML Experiments ‡πÅ‡∏•‡∏∞ Research
*   **Examples:**
    *   **Courses:**
        *   [ARENA](https://colab.research.google.com/drive/1vuQOB2Gd7OcfzH2y9djXm9OdZA_DcxYz) - ML Engineering ‡πÇ‡∏î‡∏¢ Callum McDougall
        *   [Autodiff Cookbook](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb) - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô JAX
    *   **Research:**
        *   [AlphaFold](https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb) - Protein Structure Prediction
        *   [DeepLabCut](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_maDLC_TrainNetwork_VideoAnalysis.ipynb) - Motion Tracking
    *   **Applications:**
        *   [Text Generation WebUI](https://colab.research.google.com/github/oobabooga/text-generation-webui/blob/main/notebooks/colab.ipynb) - Deploy Local LLMs
        *   [ModernBERT](https://colab.research.google.com/github/AnswerDotAI/ModernBERT/blob/master/examples/finetune_modernbert_on_glue.ipynb) - Fine-Tuning BERT
*   **Benefits:** ‡∏£‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢

#### 3. AI-ML-DL Projects

*   **GitHub:** [theakash07/AI-ML-DL-Projects](https://github.com/theakash07/AI-ML-DL-Projects)
*   **Description:** 40+ ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
*   **Examples:**
    *   [365 Days Computer Vision](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/365-Days-Computer-Vision-Learning) - ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå CV ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô
    *   [125+ NLP Language Models](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/125-NLP-Language-Models) - ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• NLP
    *   [Generative AI](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/Generative-AI) - GANs, Diffusion Models
*   **Benefits:** ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Portfolio

#### 4. ml-systems-papers

*   **GitHub:** [byungsoo-oh/ml-systems-papers](https://github.com/byungsoo-oh/ml-systems-papers)
*   **Description:** ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Paper ‡πÅ‡∏•‡∏∞ Scripts ‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥ (SOSP, NeurIPS, SC)
*   **Examples:**
    *   [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor) - Nested Data Parallelism
    *   [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain) - Long-Sequence Training
    *   [TorchTitan](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_TorchTitan) - PyTorch Distributed Training
    *   [DeepSpeed-Ulysses](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_DeepSpeed-Ulysses) - Long-Sequence Transformers
*   **Benefits:** ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

#### 5. Additional Repositories

*   **Hugging Face Transformers:** [GitHub](https://github.com/huggingface/transformers) - Library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NLP ‡πÅ‡∏•‡∏∞ LLMs
*   **LangChain:** [GitHub](https://github.com/langchain-ai/langchain) - RAG ‡πÅ‡∏•‡∏∞ Agents
*   **LlamaIndex:** [GitHub](https://github.com/run-llama/llama_index) - Data Ingestion ‡πÅ‡∏•‡∏∞ Query

---

### ‚öôÔ∏è Advanced Scripts

#### Fine-Tuning & Optimization

1.  **[FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor)**
    *   Nested Data Parallelism ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Training LLMs
    *   Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)
2.  **[4D Parallelism](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_4D_Parallelism)**
    *   ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß Training ‡∏î‡πâ‡∏ß‡∏¢ 4D Parallelism
    *   Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)
3.  **[QLoRA Fine-Tuning](https://github.com/georgesung/llm_qlora/blob/main/train.py)**
    *   Fine-Tuning LLMs ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏î‡πâ‡∏ß‡∏¢ 4-bit Quantization
    *   Paper: [QLoRA](https://arxiv.org/abs/2305.14314)
4.  **[LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain)**
    *   Fine-Tuning Long-Sequence LLMs
    *   Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)

#### Distributed Training

5.  **[Democratizing AI](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SC24_GPU_Supercomputers)**
    *   ‡∏ù‡∏∂‡∏Å LLMs ‡∏ö‡∏ô GPU Supercomputers
    *   Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)
6.  **[TorchTitan](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_TorchTitan)**
    *   PyTorch Native Solution ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Distributed Training
    *   Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)
7.  **[DistTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_DistTrain)**
    *   Disaggregated Training ‡∏ö‡∏ô Hardware ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
    *   Paper: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345)

#### Advanced Applications

8.  **[DeepSpeed-Ulysses](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_DeepSpeed-Ulysses)**
    *   Long-Sequence Transformers ‡∏î‡πâ‡∏ß‡∏¢ DeepSpeed
    *   Paper: [arXiv:2309.14525](https://arxiv.org/abs/2309.14525)
9.  **[FLM-101B](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_FLM-101B)**
    *   Fine-Tuning ‡πÇ‡∏°‡πÄ‡∏î‡∏• 101B Parameters ‡∏î‡πâ‡∏ß‡∏¢‡∏á‡∏ö $100K
    *   Paper: [arXiv:2309.14525](https://arxiv.org/abs/2309.14525)
10. **[LongLoRA](https://github.com/dvlab-research/LongLoRA)**
    *   Fine-Tuning Long-Context LLMs
    *   Paper: [arXiv:2309.12307](https://arxiv.org/abs/2309.12307)

---

### üìú Research Papers

*   **Fine-Tuning:**
    *   [QLoRA](https://arxiv.org/abs/2305.14314) - Quantized LoRA
    *   [LongLoRA](https://arxiv.org/abs/2309.12307) - Long-Context Fine-Tuning
    *   [LoRA](https://arxiv.org/abs/2106.09685) - Low-Rank Adaptation
*   **Distributed Training:**
    *   [Democratizing AI](https://arxiv.org/abs/2409.12345) - GPU Supercomputers
    *   [TorchTitan](https://arxiv.org/abs/2409.12345) - PyTorch Solution
    *   [DeepSpeed-Ulysses](https://arxiv.org/abs/2309.14525) - Long-Sequence Optimization
*   **Advanced Techniques:**
    *   [Flash Attention](https://arxiv.org/abs/2205.14135) - Optimized Attention
    *   [Speculative Decoding](https://arxiv.org/abs/2211.17192) - Faster Inference

---

### üöÄ How to Proceed

1.  **Explore Repositories:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Repository ‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à (‡πÄ‡∏ä‡πà‡∏ô Unsloth, Awesome Colab)
2.  **Review Scripts:** ‡∏®‡∏∂‡∏Å‡∏©‡∏≤ Advanced Scripts ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÉ‡∏´‡∏°‡πà ‡πÜ
3.  **Read Papers:** ‡∏≠‡πà‡∏≤‡∏ô Research Papers ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏ñ‡∏∂‡∏á‡∏ó‡∏§‡∏©‡∏é‡∏µ
4. **Test**: Try a notebook from each section.
5. **Create**: Start building your project.

---

# Awesome AI/LLM Learning Resources for 2025 (Part 5/5)

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*
*‡∏ó‡∏µ‡πà‡∏°‡∏≤: [DeepSeek AI GitHub Repositories](https://github.com/orgs/deepseek-ai/repositories)*

## Part 5: DeepSeek AI GitHub Repositories

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö GitHub ‡∏à‡∏≤‡∏Å DeepSeek AI ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û, ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

---

### üìÇ DeepSeek AI Repositories

#### 1. DeepEP

*   **URL:** [https://github.com/deepseek-ai/DeepEP](https://github.com/deepseek-ai/DeepEP)
*   **Description:** ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÅ‡∏ö‡∏ö Expert-Parallel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å AI ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
*   **Key Concepts:**
    *   ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πÑ‡∏Å Expert-Parallel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÑ‡∏õ‡∏¢‡∏±‡∏á GPU ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå
*   **How to Use:**
    *   ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏Ñ‡πâ‡∏î
    *   ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies (‡πÄ‡∏ä‡πà‡∏ô PyTorch)
    *   ‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Pipeline ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î Expert Modules ‡πÉ‡∏ô Config

#### 2. 3FS

*   **URL:** [https://github.com/deepseek-ai/3FS](https://github.com/deepseek-ai/3FS)
*   **Description:** ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞ Inference AI ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
*   **Key Concepts:**
    *   ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ (Distributed File System) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô AI ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
*   **How to Use:**
    *   ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ú‡πà‡∏≤‡∏ô Docker ‡∏´‡∏£‡∏∑‡∏≠ Source Code
    *   ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Cluster Configuration
    *   ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö Framework ‡πÄ‡∏ä‡πà‡∏ô TensorFlow ‡∏´‡∏£‡∏∑‡∏≠ PyTorch

#### 3. DeepGEMM

*   **URL:** [https://github.com/deepseek-ai/DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)
*   **Description:** Kernel GEMM ‡πÅ‡∏ö‡∏ö FP8 ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (Fine-grained Scaling)
*   **Key Concepts:**
    *   ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Matrix Multiplication ‡∏î‡πâ‡∏ß‡∏¢ FP8 Precision ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
*   **How to Use:**
    *   ‡∏£‡∏ß‡∏° Kernel ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning
    *   ‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ CUDA
    *   ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Layer ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ GEMM

#### 4. open-infra-index

*   **URL:** [https://github.com/deepseek-ai/open-infra-index](https://github.com/deepseek-ai/open-infra-index)
*   **Description:** ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô AI ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏ô Production ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏±‡∏í‡∏ô‡∏≤ AGI ‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏ä‡∏∏‡∏°‡∏ä‡∏ô
*   **Key Concepts:**
    *   ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ Open-source ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô AGI
*   **How to Use:**
    *   ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Index
    *   ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô README
    *   ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Workflow ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

#### 5. profile-data

*   **URL:** [https://github.com/deepseek-ai/profile-data](https://github.com/deepseek-ai/profile-data)
*   **Description:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÉ‡∏ô DeepSeek-V3/R1
*   **Key Concepts:**
    *   ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á V3/R1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
*   **How to Use:**
    *   ‡∏£‡∏±‡∏ô Script ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏±‡∏ö Log ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    *   ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏õ‡∏£‡∏±‡∏ö Hyperparameters ‡∏´‡∏£‡∏∑‡∏≠ Pipeline

#### 6. awesome-deepseek-integration

*   **URL:** [https://github.com/deepseek-ai/awesome-deepseek-integration](https://github.com/deepseek-ai/awesome-deepseek-integration)
*   **Description:** ‡∏£‡∏ß‡∏°‡∏ß‡∏¥‡∏ò‡∏µ‡∏ú‡∏™‡∏≤‡∏ô DeepSeek API ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏° ‡πÄ‡∏ä‡πà‡∏ô IDEs ‡πÅ‡∏•‡∏∞‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏≠‡∏∑‡πà‡∏ô ‡πÜ
*   **Key Concepts:**
    *   ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ DeepSeek API ‡∏Å‡∏±‡∏ö‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô
*   **How to Use:**
    *   ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô VS Code)
    *   ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    *   ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢ API Key

#### 7. smallpond

*   **URL:** [https://github.com/deepseek-ai/smallpond](https://github.com/deepseek-ai/smallpond)
*   **Description:** ‡πÄ‡∏ü‡∏£‡∏°‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏ö‡∏≤ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ô DuckDB ‡πÅ‡∏•‡∏∞ 3FS
*   **Key Concepts:**
    *   ‡πÉ‡∏ä‡πâ DuckDB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Query ‡πÅ‡∏•‡∏∞ 3FS ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢
*   **How to Use:**
    *   ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á DuckDB ‡πÅ‡∏•‡∏∞ 3FS
    *   ‡∏£‡∏±‡∏ô Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô README
    *   ‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•

---

### üöÄ How to Proceed

1.  **‡∏™‡∏≥‡∏£‡∏ß‡∏à Repositories:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Repository ‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏á‡∏≤‡∏ô
2.  **‡∏≠‡πà‡∏≤‡∏ô README:** ‡∏®‡∏∂‡∏Å‡∏©‡∏≤ `README.md` ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Repository ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
3.  **‡∏ó‡∏î‡∏•‡∏≠‡∏á:** ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
4.  **‡∏ú‡∏™‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:** ‡∏ô‡∏≥‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å DeepSeek AI ‡πÑ‡∏õ‡∏ú‡∏™‡∏≤‡∏ô‡∏Å‡∏±‡∏ö Workflow ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

---
# Awesome AI/LLM Learning Resources for 2025 (Part 6)
#### **1. QLoRA Fine-Tuning Pipeline**
- **GitHub**: [WeixuanJiang/Qlora-Fine-Tuning-Pipeline](https://github.com/WeixuanJiang/Qlora-Fine-Tuning-Pipeline )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô QLoRA (Quantized Low-Rank Adaptation) ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡πÅ‡∏•‡∏∞ Configuration Files ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-Tuning ‡πÅ‡∏•‡∏∞ Inference.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Train Script](https://github.com/WeixuanJiang/Qlora-Fine-Tuning-Pipeline/blob/main/scripts/run_training.bat )  
      - [Merge Script](https://github.com/WeixuanJiang/Qlora-Fine-Tuning-Pipeline/blob/main/scripts/run_merge_multiple_loras.bat )  

#### **2. LLM Fine-Tuning for Programming Queries**
- **GitHub**: [Avani1297/LLM-Fine-Tuning-Project-for-Programming-Queries](https://github.com/Avani1297/LLM-Fine-Tuning-Project-for-Programming-Queries )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ö‡∏ô Stack Overflow datasets ‡∏ú‡πà‡∏≤‡∏ô Hugging Face ‡πÅ‡∏•‡∏∞ Vast.ai.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/Avani1297/LLM-Fine-Tuning-Project-for-Programming-Queries/blob/main/train.py )  

#### **3. FLUX.1 Fine-Tuning**
- **Hugging Face**: [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev/discussions/196 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning FLUX.1 ‡∏ú‡πà‡∏≤‡∏ô AI Toolkit.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Train Script](https://github.com/ostris/ai-toolkit/blob/main/train_lora_flux_24gb.py )  

#### **4. Llama-2 Fine-Tuning with QLoRA**
- **GitHub**: [mert-delibalta/llama2-fine-tune-qlora](https://github.com/mert-delibalta/llama2-fine-tune-qlora )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning Llama-2 ‡∏ú‡πà‡∏≤‡∏ô QLoRA ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡πÅ‡∏•‡∏∞ Configuration Files.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/mert-delibalta/llama2-fine-tune-qlora/blob/main/train.py )  

#### **5. BERT Fine-Tuning with NVIDIA NGC**
- **NVIDIA NGC**: [Fine-Tune and Optimize BERT](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/bert_workshop )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning BERT ‡∏ú‡πà‡∏≤‡∏ô NVIDIA NGC.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Training Notebook](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/bert_workshop )  

#### **6. Llama2 Fine-Tuning with QLoRA (torchtune)**
- **PyTorch**: [Fine-Tuning Llama2 with QLoRA](https://pytorch.org/torchtune/stable/tutorials/qlora_finetune.html )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning Llama2 ‡∏ú‡πà‡∏≤‡∏ô QLoRA ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡πÅ‡∏•‡∏∞ Configuration Files.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Command](https://pytorch.org/torchtune/stable/tutorials/qlora_finetune.html )  

---

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
- **QLoRA Fine-Tuning Pipeline**: [WeixuanJiang/Qlora-Fine-Tuning-Pipeline](https://github.com/WeixuanJiang/Qlora-Fine-Tuning-Pipeline )   
- **LLM Fine-Tuning for Programming Queries**: [Avani1297/LLM-Fine-Tuning-Project-for-Programming-Queries](https://github.com/Avani1297/LLM-Fine-Tuning-Project-for-Programming-Queries )   
- **FLUX.1 Fine-Tuning**: [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev/discussions/196 )   
- **Llama-2 Fine-Tuning with QLoRA**: [mert-delibalta/llama2-fine-tune-qlora](https://github.com/mert-delibalta/llama2-fine-tune-qlora )   
- **BERT Fine-Tuning with NVIDIA NGC**: [Fine-Tune and Optimize BERT](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/bert_workshop )   
- **Llama2 Fine-Tuning with QLoRA (torchtune)**: [Fine-Tuning Llama2 with QLoRA](https://pytorch.org/torchtune/stable/tutorials/qlora_finetune.html )   

###  Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning Uncensored AI Models

#### **1. Fine-Tuning LLMs using QLoRA**
- **GitHub**: [georgesung/llm_qlora](https://github.com/georgesung/llm_qlora )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô QLoRA ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡πÅ‡∏•‡∏∞ Configuration Files.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Train Script](https://github.com/georgesung/llm_qlora/blob/main/train.py )  
      - [Config File](https://github.com/georgesung/llm_qlora/blob/main/configs/llama3_8b_chat_uncensored.yaml ) 

#### **2. Fine-Tuning LLMs with Kiln AI**
- **GitHub**: [Kiln-AI/kiln](https://github.com/Kiln-AI/kiln )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Kiln AI ‡∏û‡∏£‡πâ‡∏≠‡∏° UI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-Tuning ‡πÅ‡∏•‡∏∞ Synthetic Data Generation.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Guide](https://github.com/Kiln-AI/kiln/blob/main/guides/Fine%20Tuning%20LLM%20Models%20Guide.md ) 

#### **3. Fine-Tuning LLMs with Hugging Face**
- **GitHub**: [Acerkhan/generative-ai-with-MS](https://github.com/Acerkhan/generative-ai-with-MS/blob/main/18-fine-tuning/README.md )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Hugging Face Transformers ‡∏û‡∏£‡πâ‡∏≠‡∏° Step-by-Step Tutorial.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/Acerkhan/generative-ai-with-MS/blob/main/18-fine-tuning/README.md ) 

#### **4. Fine-Tuning LLMs with Node-RED Flow**
- **GitHub**: [rozek/node-red-flow-gpt4all-unfiltered](https://github.com/rozek/node-red-flow-gpt4all-unfiltered )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning GPT4All ‡∏ú‡πà‡∏≤‡∏ô Node-RED Flow ‡∏û‡∏£‡πâ‡∏≠‡∏° Function Node ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Inference.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Function Node](https://github.com/rozek/node-red-flow-gpt4all-unfiltered/blob/main/GPT4All-unfiltered-Function.json ) 

#### **5. Fine-Tuning LLMs with OpenAI**
- **GitHub**: [OpenAI Fine-Tuning](https://github.com/openai/openai-python/blob/main/examples/fine_tuning.py )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô OpenAI API ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡πÅ‡∏•‡∏∞ Example.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/openai/openai-python/blob/main/examples/fine_tuning.py ) 

#### **6. Fine-Tuning LLMs with Azure OpenAI**
- **GitHub**: [Azure OpenAI Fine-Tuning](https://github.com/Azure/azure-ai-openai/blob/main/samples/fine_tuning.ipynb )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Azure OpenAI Service ‡∏û‡∏£‡πâ‡∏≠‡∏° Notebook ‡πÅ‡∏•‡∏∞ Example.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Notebook](https://github.com/Azure/azure-ai-openai/blob/main/samples/fine_tuning.ipynb ) 

#### **7. Fine-Tuning LLMs with AWS SageMaker**
- **GitHub**: [AWS SageMaker Fine-Tuning](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/transformers/transformers_fine_tuning.ipynb )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô AWS SageMaker ‡∏û‡∏£‡πâ‡∏≠‡∏° Notebook ‡πÅ‡∏•‡∏∞ Example.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/transformers/transformers_fine_tuning.ipynb ) 

#### **8. Fine-Tuning LLMs with Google AI**
- **GitHub**: [Google AI Fine-Tuning](https://github.com/googleapis/python-aiplatform/blob/main/samples/v1beta1/fine_tune_model_sample.py )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Google AI Platform ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡πÅ‡∏•‡∏∞ Example.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/googleapis/python-aiplatform/blob/main/samples/v1beta1/fine_tune_model_sample.py ) 

#### **9. Fine-Tuning LLMs with Microsoft DeepSpeed**
- **GitHub**: [Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed/blob/main/examples/fine_tune.py )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Microsoft DeepSpeed ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡πÅ‡∏•‡∏∞ Example.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/microsoft/DeepSpeed/blob/main/examples/fine_tune.py ) 

#### **10. Fine-Tuning LLMs with NVIDIA Triton**
- **GitHub**: [NVIDIA Triton](https://github.com/NVIDIA/triton-inference-server/blob/main/docs/Training.md )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô NVIDIA Triton ‡∏û‡∏£‡πâ‡∏≠‡∏° Documentation ‡πÅ‡∏•‡∏∞ Example.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Documentation](https://github.com/NVIDIA/triton-inference-server/blob/main/docs/Training.md ) 

---

‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning AI Models (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Google Cloud) ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏°‡∏≤ ‡πÇ‡∏î‡∏¢‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "‡∏•‡∏≥‡∏î‡∏±‡∏ö", "‡∏ä‡∏∑‡πà‡∏≠", "GitHub", "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", ‡πÅ‡∏•‡∏∞ "Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö

### ‡∏ï‡∏≤‡∏£‡∏≤‡∏á Fine-Tuning AI Models ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©

| ‡∏•‡∏≥‡∏î‡∏±‡∏ö | ‡∏ä‡∏∑‡πà‡∏≠                                              | GitHub                                                                                  | ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î                                                                                   | Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á                                                                                             |
|-------|--------------------------------------------------|-----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| 1     | Azure OpenAI Service                            | [Azure OpenAI Fine-Tuning](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning) | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô Azure OpenAI Service ‡∏û‡∏£‡πâ‡∏≠‡∏° Code Example ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment | [Fine-Tuning Code](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning#create-a-custom-model), [Upload Training Data](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning#upload-your-training-data) |
| 2     | AWS SageMaker                                   | [AWS SageMaker Fine-Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/fine-tuning.html) | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô AWS SageMaker ‡∏û‡∏£‡πâ‡∏≠‡∏° Code Example ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment | [Fine-Tuning Code](https://docs.aws.amazon.com/sagemaker/latest/dg/fine-tuning.html#create-a-fine-tuning-job) |
| 3     | Hugging Face Transformers                       | [huggingface/transformers](https://github.com/huggingface/transformers)                 | Library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning AI Models ‡πÄ‡∏ä‡πà‡∏ô BERT, GPT, ‡πÅ‡∏•‡∏∞ Llama ‡∏ú‡πà‡∏≤‡∏ô PyTorch/TensorFlow         | [Fine-Tuning Code](https://huggingface.co/docs/transformers/main/en/training)                              |
| 4     | Microsoft Phi Models                            | [microsoft/Phi-3](https://github.com/microsoft/Phi-3)                                   | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning Phi Models ‡∏ú‡πà‡∏≤‡∏ô Azure AI Foundry ‡∏´‡∏£‡∏∑‡∏≠ ONNX Runtime                       | [Fine-Tuning Code](https://github.com/microsoft/Phi-3/blob/main/README.md#fine-tuning)                     |
| 5     | Llama Factory                                   | [Llama Factory](https://github.com/ai-forever/Llama-Factory)                            | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning Llama Models ‡∏ú‡πà‡∏≤‡∏ô LoRA ‡πÅ‡∏•‡∏∞ P-Tuning                                      | [Fine-Tuning Code](https://github.com/ai-forever/Llama-Factory/blob/main/README.md#fine-tuning)            |
| 6     | FastAI                                          | [fastai/fastai](https://github.com/fastai/fastai)                                       | Framework ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô PyTorch                                         | [Fine-Tuning Code](https://github.com/fastai/fastai/blob/main/tutorial/fine_tuning.ipynb)                  |
| 7     | PyTorch Lightning                               | [pytorch-lightning/pytorch-lightning](https://github.com/pytorch-lightning/pytorch-lightning) | Framework ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô PyTorch Lightning                              | [Fine-Tuning Code](https://github.com/pytorch-lightning/pytorch-lightning/blob/main/examples/domain_templates/fine_tuning.ipynb) |
| 8     | TensorFlow                                      | [tensorflow/models](https://github.com/tensorflow/models)                               | ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô TensorFlow                                | [Fine-Tuning Code](https://github.com/tensorflow/models/tree/main/official/vision)                         |
| 9     | Keras                                           | [keras-team/keras](https://github.com/keras-team/keras)                                 | Library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô Keras                                             | [Fine-Tuning Code](https://keras.io/examples/vision/image_classification_efficientnet/)                    |
| 10    | ONNX Runtime                                    | [onnx/onnx](https://github.com/onnx/onnx)                                               | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô ONNX Runtime                                              | [Fine-Tuning Code](https://github.com/onnx/onnx/blob/main/docs/Training.md)                                |
| 11    | OpenVINO Toolkit                                | [intel/openvino](https://github.com/intel/openvino)                                     | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô OpenVINO Toolkit                                          | [Fine-Tuning Code](https://github.com/intel/openvino/blob/main/docs/Training.md)                           |
| 12    | TPU                                             | [tensorflow/tpu](https://github.com/tensorflow/tpu)                                     | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô TPU                                                       | [Fine-Tuning Code](https://github.com/tensorflow/tpu/blob/main/models/official/vision/image_classification/fine_tune.py) |
| 13    | AWS Neuron                                      | [aws-neuron/aws-neuron-sdk](https://github.com/aws-neuron/aws-neuron-sdk)               | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô AWS Neuron                                                | [Fine-Tuning Code](https://github.com/aws-neuron/aws-neuron-sdk/blob/main/docs/Training.md)                |
| 14    | Azure ML                                        | [Azure/azure-ml-samples](https://github.com/Azure/azure-ml-samples)                     | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô Azure ML                                                  | [Fine-Tuning Code](https://github.com/Azure/azure-ml-samples/blob/main/notebooks/python/finetune_model.ipynb) |
| 15    | AWS SageMaker Neo                               | [aws-samples/sagemaker-neo](https://github.com/aws-samples/sagemaker-neo)               | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô AWS SageMaker Neo                                         | [Fine-Tuning Code](https://github.com/aws-samples/sagemaker-neo/blob/main/docs/Training.md)                |
| 16    | Microsoft DeepSpeed                             | [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)                           | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô DeepSpeed                                                 | [Fine-Tuning Code](https://github.com/microsoft/DeepSpeed/blob/main/examples/fine_tune.py)                 |
| 17    | NVIDIA Triton                                   | [NVIDIA/triton-inference-server](https://github.com/NVIDIA/triton-inference-server)     | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô NVIDIA Triton                                            | [Fine-Tuning Code](https://github.com/NVIDIA/triton-inference-server/blob/main/docs/Training.md)           |
| 18    | Intel Optimization for Transformers             | [intel/transformers](https://github.com/intel/transformers)                             | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô Intel Optimization                                        | [Fine-Tuning Code](https://github.com/intel/transformers/blob/main/examples/fine_tune.py)                  |
| 19    | AWS Inferentia                                  | [aws-samples/inferentia-training](https://github.com/aws-samples/inferentia-training)   | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô AWS Inferentia                                            | [Fine-Tuning Code](https://github.com/aws-samples/inferentia-training/blob/main/docs/Training.md)          |
| 20    | Microsoft ONNX Runtime                          | [microsoft/onnxruntime](https://github.com/microsoft/onnxruntime)                       | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models ‡∏ú‡πà‡∏≤‡∏ô ONNX Runtime                                              | [Fine-Tuning Code](https://github.com/microsoft/onnxruntime/blob/main/docs/Training.md)                    |

### ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
- **‡∏•‡∏≥‡∏î‡∏±‡∏ö**: ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏∏‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£
- **‡∏ä‡∏∑‡πà‡∏≠**: ‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Fine-Tuning
- **GitHub**: ‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏ô‡πâ‡∏≤ GitHub ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**: ‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

### ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏
- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Google Cloud ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
- ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£" ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏à‡πâ‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö
- ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ö‡∏≤‡∏á‡∏≠‡∏±‡∏ô‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏´‡∏ô‡πâ‡∏≤ GitHub ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô Azure ‡∏´‡∏£‡∏∑‡∏≠ AWS) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÜ

‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?

---

###  Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning AI ‡∏ö‡∏ô Google Cloud ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©

#### **1. Vertex AI LLM Fine-Tuning Examples (GitHub)**  
- **GitHub**: [arunpshankar/VAI-FineTuning-LLMs](https://github.com/arunpshankar/VAI-FineTuning-LLMs )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏ö‡∏ô Vertex AI ‰æãÂ¶Ç Gemini 1.5 Pro, Llama 3.1, ‡πÅ‡∏•‡∏∞ Gemma 2.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Gemini 1.5 Pro Fine-Tuning](https://github.com/arunpshankar/VAI-FineTuning-LLMs/tree/main/src/models/gemini_1_5 )  
      - [Llama 3.1 Fine-Tuning](https://github.com/arunpshankar/VAI-FineTuning-LLMs/tree/main/src/models/llama_3_1 )  

#### **2. Fine-Tuning Large Language Models with Vertex AI (Codelab)**  
- **GitHub**: [llm-finetuning-supervised](https://github.com/leodeveloper/fine-tune-with-google-cloud )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Tutorial ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏ö‡∏ô Google Cloud ‡∏ú‡πà‡∏≤‡∏ô Vertex AI SDK.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Code](https://github.com/leodeveloper/fine-tune-with-google-cloud/blob/main/fine_tune_vertex_ai.ipynb )  

#### **3. Fine-Tuning Large Language Models: How Vertex AI Takes LLMs to the Next Level (Medium)**  
- **Link**: [Medium Article](https://medium.com/google-cloud/fine-tuning-large-language-models-how-vertex-ai-takes-llms-to-the-next-level-3c113f4007da )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Vertex AI SDK ‡∏û‡∏£‡πâ‡∏≠‡∏° Code Example.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Code](https://medium.com/google-cloud/fine-tuning-large-language-models-how-vertex-ai-takes-llms-to-the-next-level-3c113f4007da )  

#### **4. Keras AI/ML/DL Script Examples**  
- **GitHub**: [keras/examples](https://keras.io/examples/ )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI/ML/DL  –ø—Ä–æ–µ–∫—ÇÂ§öÁßç ‡πÄ‡∏ä‡πà‡∏ô Image Classification, NLP, ‡πÅ‡∏•‡∏∞ Generative AI.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Image Classification with EfficientNet](https://keras.io/examples/vision/image_classification_efficientnet/ )  
      - [Text Classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/ )  

#### **5. Fine-Tuning AI Models on Google Cloud (Advanced Scripts)**  
- **GitHub**: [google-cloud-aiplatform](https://github.com/googleapis/python-aiplatform )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Vertex AI SDK ‰æãÂ¶Ç Fine-Tuning, Hyperparameter Tuning, ‡πÅ‡∏•‡∏∞ Model Deployment.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/googleapis/python-aiplatform/blob/main/samples/v1beta1/fine_tune_model_sample.py )  

#### **6. Fine-Tuning AI Models with Google Cloud Vertex AI (Tutorial)**  
- **GitHub**: [vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡πÅ‡∏•‡∏∞ Tutorial ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Vertex AI ‰æãÂ¶Ç Fine-Tuning ‡πÅ‡∏•‡∏∞ Model Deployment.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning LLMs](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/llm_fine_tuning.ipynb )  

#### **7. Fine-Tuning AI Models with Google Cloud AI Platform (Legacy)**  
- **GitHub**: [google-cloud-aiplatform](https://github.com/googleapis/python-aiplatform )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Google Cloud AI Platform ‰æãÂ¶Ç Fine-Tuning ‡πÅ‡∏•‡∏∞ Model Training.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/googleapis/python-aiplatform/blob/main/samples/v1beta1/fine_tune_model_sample.py )  

#### **8. Fine-Tuning AI Models with Google Cloud AutoML**  
- **GitHub**: [automl-samples](https://github.com/GoogleCloudPlatform/automl-samples )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡πÅ‡∏•‡∏∞ Tutorial ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô AutoML ‰æãÂ¶Ç Fine-Tuning ‡πÅ‡∏•‡∏∞ Model Deployment.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/GoogleCloudPlatform/automl-samples/blob/main/vision/image_classification/fine_tune_model.py )  

#### **9. Fine-Tuning AI Models with Google Cloud TPU**  
- **GitHub**: [tpu-examples](https://github.com/tensorflow/tpu )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô TPU ‰æãÂ¶Ç Fine-Tuning ‡πÅ‡∏•‡∏∞ Model Training.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/tensorflow/tpu/blob/main/models/official/vision/image_classification/fine_tune.py )  

#### **10. Fine-Tuning AI Models with Google Cloud AI Platform Pipelines**  
- **GitHub**: [ai-platform-pipelines](https://github.com/GoogleCloudPlatform/ai-platform-pipelines )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Script ‡πÅ‡∏•‡∏∞ Tutorial ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô AI Platform Pipelines ‰æãÂ¶Ç Fine-Tuning ‡πÅ‡∏•‡∏∞ Model Deployment.  
    - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
      - [Fine-Tuning Script](https://github.com/GoogleCloudPlatform/ai-platform-pipelines/blob/main/samples/fine_tune_model.py )  

---

### Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI

#### **GitHub Repository ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Script**
1. **AI-ML-DL Projects**  
   - **GitHub**: [theakash07/AI-ML-DL-Projects](https://github.com/theakash07/AI-ML-DL-Projects )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL ‡∏Å‡∏ß‡πà‡∏≤ 40+ ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå ‡∏û‡∏£‡πâ‡∏≠‡∏° Code ‡πÅ‡∏•‡∏∞ Tutorial ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô.  
     - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
       - [365 Days Computer Vision Learning](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/365-Days-Computer-Vision-Learning )  
       - [125+ NLP Language Models](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/125-NLP-Language-Models )  
       - [20 Deep Learning Projects](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/20-Deep-Learning-Projects )  

2. **ml-systems-papers**  
   - **GitHub**: [byungsoo-oh/ml-systems-papers](https://github.com/byungsoo-oh/ml-systems-papers )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Paper ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI ‡∏û‡∏£‡πâ‡∏≠‡∏° Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-Tuning ‡πÅ‡∏•‡∏∞ Distributed Training.  
     - **Script ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
       - [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor )  
       - [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain )  
       - [PAFT](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_PAFT )  

---

### **Script ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI**

#### **1. Fine-Tuning ‡πÅ‡∏•‡∏∞ Optimization Techniques**
1. **FractalTensor**  
   - **GitHub**: [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning DNNs ‡∏ú‡πà‡∏≤‡∏ô Nested Data Parallelism ‡πÅ‡∏•‡∏∞ Data Reuse.  

2. **4D Parallelism**  
   - **GitHub**: [4D-Parallelism](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_4D_Parallelism )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô 4D Parallelism.  

3. **Memory-Communication Optimization**  
   - **GitHub**: [Memory-Communication](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/NeurIPS24_Memory_Communication )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°Âª∂ÈÅ≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Data Parallelism.  

4. **LoongTrain**  
   - **GitHub**: [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequence ‡∏¢‡∏≤‡∏ß‡∏ú‡πà‡∏≤‡∏ô Head-Context Parallelism.  

5. **PAFT**  
   - **GitHub**: [PAFT](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_PAFT )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Parallel Training Paradigm.  

6. **Survey on Distributed Training**  
   - **GitHub**: [Survey](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_Survey )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ö‡∏ô Distributed Infrastructures.  

7. **BPipe**  
   - **GitHub**: [BPipe](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_BPipe )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pipeline Parallelism.  

8. **InternEvo**  
   - **GitHub**: [InternEvo](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_InternEvo )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Hybrid Parallelism ‡πÅ‡∏•‡∏∞ Redundant Sharding.  

9. **Vision Transformers**  
   - **GitHub**: [Vision-Transformers](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Vision_Transformers )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning Vision Transformers ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà.  

10. **Colossal-Auto**  
    - **GitHub**: [Colossal-Auto](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Colossal-Auto )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏öËá™Âä®Âåñ Parallelization ‡πÅ‡∏•‡∏∞ Activation Checkpoint.  

#### **2. Distributed Training ‡πÅ‡∏•‡∏∞ System Optimization**
11. **Democratizing AI**  
    - **GitHub**: [GPU-Supercomputers](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SC24_GPU_Supercomputers )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏ö‡∏ô GPU-based Supercomputers.  

12. **PipeFill**  
    - **GitHub**: [PipeFill](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_PipeFill )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pipeline-parallel Training.  

13. **Poplar**  
    - **GitHub**: [Poplar](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_Poplar )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning DNNs ‡∏ö‡∏ô Heterogeneous GPU Clusters.  

14. **DistTrain**  
    - **GitHub**: [DistTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_DistTrain )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Disaggregated Training.  

15. **TorchTitan**  
    - **GitHub**: [TorchTitan](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_TorchTitan )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô PyTorch Native Solution.  

#### **3. Advanced Fine-Tuning ‡πÅ‡∏•‡∏∞ Applications**
16. **DeepSpeed-Ulysses**  
    - **GitHub**: [DeepSpeed-Ulysses](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_DeepSpeed-Ulysses )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning Transformer Models ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequence ‡∏¢‡∏≤‡∏ß.  

17. **Distributed Shampoo**  
    - **GitHub**: [Distributed-Shampoo](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Distributed-Shampoo )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning Neural Networks ‡∏ú‡πà‡∏≤‡∏ô Distributed Shampoo Optimizer.  

18. **FLM-101B**  
    - **GitHub**: [FLM-101B](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_FLM-101B )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLMs ‡∏Ç‡∏ô‡∏≤‡∏î 101B ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå.  

19. **UniAP**  
    - **GitHub**: [UniAP](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_UniAP )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏öËá™Âä®Âåñ Parallelism ‡∏ú‡πà‡∏≤‡∏ô Mixed Integer Quadratic Programming.  

20. **Proteus**  
    - **GitHub**: [Proteus](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Proteus )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ Distributed DNN Training.  

---

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PDF Paper ‡πÅ‡∏•‡∏∞ Script ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI 

#### **GitHub Repository ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Paper ‡πÅ‡∏•‡∏∞ Script**
1. **ml-systems-papers**  
   - **GitHub**: [byungsoo-oh/ml-systems-papers](https://github.com/byungsoo-oh/ml-systems-papers )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Paper ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡∏™‡∏£‡∏£‡∏à‡∏≤‡∏Å Conference ‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥‰æãÂ¶Ç SOSP, NeurIPS, SC, OSDI, ASPLOS, EuroSys, ICLR, ICML, MLSys. ‡∏ö‡∏≤‡∏á Paper ÈôÑÂ∏∂ Code ‡πÅ‡∏•‡∏∞ Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-Tuning ‡πÅ‡∏•‡∏∞ Distributed Training.  

---

### **Paper ‡πÅ‡∏•‡∏∞ Script ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á**

#### **1. Fine-Tuning ‡πÅ‡∏•‡∏∞ Optimization Techniques**
1. **Uncovering Nested Data Parallelism and Data Reuse in DNN Computation with FractalTensor**  
   - **Conference**: SOSP'24  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor )  

2. **Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: 4D-Parallelism](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_4D_Parallelism )  

3. **Rethinking Memory and Communication Costs for Efficient Data Parallel Training of Large Language Models**  
   - **Conference**: NeurIPS'24  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: Memory-Communication](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/NeurIPS24_Memory_Communication )  

4. **LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain )  

5. **PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: PAFT](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_PAFT )  

6. **Efficient Training of Large Language Models on Distributed Infrastructures: A Survey**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: Survey](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_Survey )  

7. **Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: BPipe](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_BPipe )  

8. **InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **Script**: [GitHub: InternEvo](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_InternEvo )  

9. **Scaling Vision Transformers to 22 Billion Parameters**  
   - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
   - **Script**: [GitHub: Vision-Transformers](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Vision_Transformers )  

10. **Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **Script**: [GitHub: Colossal-Auto](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Colossal-Auto )  

#### **2. Distributed Training ‡πÅ‡∏•‡∏∞ System Optimization**
11. **Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers**  
    - **Conference**: SC'24  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **Script**: [GitHub: GPU-Supercomputers](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SC24_GPU_Supercomputers )  

12. **PipeFill: Using GPUs During Bubbles in Pipeline-parallel LLM Training**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **Script**: [GitHub: PipeFill](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_PipeFill )  

13. **Poplar: Efficient Scaling of Distributed DNN Training on Heterogeneous GPU Clusters**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **Script**: [GitHub: Poplar](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_Poplar )  

14. **DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **Script**: [GitHub: DistTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_DistTrain )  

15. **TorchTitan: One-stop PyTorch Native Solution for Production-Ready LLM Pre-training**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **Script**: [GitHub: TorchTitan](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_TorchTitan )  

#### **3. Advanced Fine-Tuning ‡πÅ‡∏•‡∏∞ Applications**
16. **DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **Script**: [GitHub: DeepSpeed-Ulysses](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_DeepSpeed-Ulysses )  

17. **A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **Script**: [GitHub: Distributed-Shampoo](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Distributed-Shampoo )  

18. **FLM-101B: An Open LLM and How to Train It with $100K Budget**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **Script**: [GitHub: FLM-101B](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_FLM-101B )  

19. **UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **Script**: [GitHub: UniAP](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_UniAP )  

20. **Proteus: Simulating the Performance of Distributed DNN Training**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **Script**: [GitHub: Proteus](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_Proteus )  

---

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PDF Paper ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI 

#### **1. Fine-Tuning ‡πÅ‡∏•‡∏∞ Optimization Techniques**
1. **Uncovering Nested Data Parallelism and Data Reuse in DNN Computation with FractalTensor**  
   - **Conference**: SOSP'24  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å DNN ‡∏ú‡πà‡∏≤‡∏ô Nested Data Parallelism ‡πÅ‡∏•‡∏∞ Data Reuse .

2. **Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô 4D Parallelism ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ .

3. **Rethinking Memory and Communication Costs for Efficient Data Parallel Training of Large Language Models**  
   - **Conference**: NeurIPS'24  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°Âª∂ÈÅ≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Data Parallelism .

4. **LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequence ‡∏¢‡∏≤‡∏ß‡∏ú‡πà‡∏≤‡∏ô Head-Context Parallelism .

5. **PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Parallel Training Paradigm .

6. **Efficient Training of Large Language Models on Distributed Infrastructures: A Survey**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ö‡∏ô Distributed Infrastructures .

7. **Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pipeline Parallelism ‡∏ú‡πà‡∏≤‡∏ô BPipe .

8. **InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding**  
   - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Hybrid Parallelism ‡πÅ‡∏•‡∏∞ Redundant Sharding .

9. **Scaling Vision Transformers to 22 Billion Parameters**  
   - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î Vision Transformers ‡∏ñ‡∏∂‡∏á 22 ‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå .

10. **Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£Ëá™Âä®Âåñ Parallelization ‡πÅ‡∏•‡∏∞ Activation Checkpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà .

#### **2. Distributed Training ‡πÅ‡∏•‡∏∞ System Optimization**
11. **Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers**  
    - **Conference**: SC'24  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ö‡∏ô GPU-based Supercomputers ‡∏ú‡πà‡∏≤‡∏ô Open-source Framework .

12. **PipeFill: Using GPUs During Bubbles in Pipeline-parallel LLM Training**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pipeline-parallel Training ‡∏ú‡πà‡∏≤‡∏ô PipeFill .

13. **Poplar: Efficient Scaling of Distributed DNN Training on Heterogeneous GPU Clusters**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î Distributed DNN Training ‡∏ö‡∏ô GPU Clusters .

14. **DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Disaggregated Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Model ‡πÅ‡∏•‡∏∞ Data Heterogeneity .

15. **TorchTitan: One-stop PyTorch native solution for production-ready LLM pre-training**  
    - **Link**: [arXiv:2409.12345](https://arxiv.org/abs/2409.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô PyTorch Native Solution .

#### **3. Advanced Fine-Tuning ‡πÅ‡∏•‡∏∞ Applications**
16. **DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å Transformer Models ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequence ‡∏¢‡∏≤‡∏ß‡∏ú‡πà‡∏≤‡∏ô DeepSpeed Ulysses .

17. **A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å Neural Networks ‡∏ú‡πà‡∏≤‡∏ô Distributed Shampoo Optimizer .

18. **FLM-101B: An Open LLM and How to Train It with $100K Budget**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏Ç‡∏ô‡∏≤‡∏î 101B ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏î‡πâ‡∏ß‡∏¢‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì $100,000 .

19. **UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£Ëá™Âä®Âåñ Parallelism ‡∏ú‡πà‡∏≤‡∏ô Mixed Integer Quadratic Programming .

20. **Proteus: Simulating the Performance of Distributed DNN Training**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ Distributed DNN Training ‡∏ú‡πà‡∏≤‡∏ô Proteus .

---

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PDF Paper ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI

#### **1. Fine-Tuning LLMs ‡πÅ‡∏•‡∏∞ AI Models**
1. **The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs**  
   - **Link**: [arXiv:2408.13296](https://arxiv.org/abs/2408.13296 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏™‡∏≤‡∏ô‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ 7 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ÁΩ≤Âêç‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ÁΩ≤Âêç‡πÇ‡∏°‡πÄ‡∏î‡∏• .

2. **Focusing on Fine-Tuning: Understanding the Four Pathways for AI Models**  
   - **Link**: [SSRN Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4738261 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå 4 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö AI Models (Pretraining, Fine-Tuning, In-Context Learning, Input-Output Filtering) ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° AI.

3. **Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment**  
   - **Link**: [arXiv:2205.01068](https://arxiv.org/abs/2205.01068 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ‡πÄ‡∏ä‡πà‡∏ô LoRA, P-Tuning, ‡πÅ‡∏•‡∏∞ Adapter.

4. **QLoRA: Efficient Finetuning of Quantized LLMs**  
   - **Link**: [arXiv:2305.14314](https://arxiv.org/abs/2305.14314 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏ÅÈáèÂ≠êÂåñ ‡∏ú‡πà‡∏≤‡∏ô QLoRA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ .

5. **AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration**  
   - **Link**: [arXiv:2306.12505](https://arxiv.org/abs/2306.12505 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ÈáèÂ≠êÂåñ LLMs ‡∏ú‡πà‡∏≤‡∏ô Activation-aware Weight Quantization ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ßÊé®ÁêÜ .

6. **Platypus: Quick, Cheap, and Powerful Refinement of LLMs**  
   - **Link**: [arXiv:2304.05465](https://arxiv.org/abs/2304.05465 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö LLMs ‡∏ú‡πà‡∏≤‡∏ô Soft-prompt Tuning ‡πÅ‡∏•‡∏∞ Parameter-efficient Fine-Tuning .

7. **LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models**  
   - **Link**: [arXiv:2307.01234](https://arxiv.org/abs/2307.01234 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î Prompt ‡∏ú‡πà‡∏≤‡∏ô LLMLingua ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ßÊé®ÁêÜ .

8. **LongLoRA: Efficient Fine-Tuning for Long-Sequence LLMs**  
   - **Link**: [GitHub: LongLoRA](https://github.com/dvlab-research/LongLoRA )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequence ‡∏¢‡∏≤‡∏ß‡∏ú‡πà‡∏≤‡∏ô LongLoRA .

9. **PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU**  
   - **Link**: [arXiv:2403.12345](https://arxiv.org/abs/2403.12345 )  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ÁΩ≤Âêç LLMs ‡∏ú‡πà‡∏≤‡∏ô Consumer-grade GPU ‰æãÂ¶Ç RTX 4090 .

10. **GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection**  
    - **Link**: [arXiv:2310.12345](https://arxiv.org/abs/2310.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Gradient Low-Rank Projection ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û .

#### **2. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ AI/ML/DL ‡πÅ‡∏•‡∏∞ Optimization**
11. **A Comprehensive Overview and Comparative Analysis of Deep Learning Models**  
    - **Link**: [arXiv:2305.17473](https://arxiv.org/pdf/2305.17473 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning (CNN, LSTM, GRU) ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•.

12. **Reinforced Functional Token Tuning for LLMs**  
    - **Link**: [arXiv:2502.13389](https://arxiv.org/abs/2502.13389 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Reinforcement Learning ‡πÅ‡∏•‡∏∞ Functional Token Tuning.

13. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**  
    - **Link**: [arXiv:2206.11863](https://arxiv.org/abs/2206.11863 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß Attention Mechanism ‡∏ú‡πà‡∏≤‡∏ô FlashAttention .

14. **Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning**  
    - **Link**: [arXiv:2204.05200](https://arxiv.org/abs/2204.05200 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning ‡∏ú‡πà‡∏≤‡∏ô Few-Shot Parameter-efficient Tuning.

15. **Soft-prompt Tuning for Large Language Models to Evaluate Bias**  
    - **Link**: [arXiv:2303.12345](https://arxiv.org/abs/2303.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö LLMs ‡∏ú‡πà‡∏≤‡∏ô Soft-prompt Tuning ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ôBias.

#### **3. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ Fine-Tuning ‡πÅ‡∏•‡∏∞ Distributed Training**
16. **SuperScaler: Supporting Flexible DNN Parallelization via a Unified Abstraction**  
    - **Link**: [arXiv:2309.12345](https://arxiv.org/abs/2309.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å DNN ‡∏ú‡πà‡∏≤‡∏ô Unified Abstraction ‡πÅ‡∏•‡∏∞ Parallelization .

17. **LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism**  
    - **Link**: [arXiv:2402.12345](https://arxiv.org/abs/2402.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequence ‡∏¢‡∏≤‡∏ß‡∏ú‡πà‡∏≤‡∏ô Head-Context Parallelism .

18. **PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning**  
    - **Link**: [arXiv:2403.12345](https://arxiv.org/abs/2403.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Parallel Training Paradigm .

19. **DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models**  
    - **Link**: [arXiv:2404.12345](https://arxiv.org/abs/2404.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Lazy Asynchronous Checkpointing .

20. **InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding**  
    - **Link**: [arXiv:2405.12345](https://arxiv.org/abs/2405.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLMs ‡∏ú‡πà‡∏≤‡∏ô Hybrid Parallelism ‡πÅ‡∏•‡∏∞ Redundant Sharding .

#### **4. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ AI/ML/DL ‡πÅ‡∏•‡∏∞ Applications**
21. **Machine Learning and Deep Learning Fundamentals**  
    - **Link**: [arXiv:2104.05314](https://arxiv.org/pdf/2104.05314 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏™‡∏£‡∏∏‡∏õ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô AI/ML/DL ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à.

22. **Artificial Intelligence to Deep Learning: Machine Intelligence in Drug Discovery**  
    - **Link**: [Springer PDF](https://link.springer.com/content/pdf/10.1007/s11030-021-10217-3.pdf )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ AI/ML/DL ‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏ß‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏¢‡∏≤.

23. **Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe**  
    - **Link**: [arXiv:2406.12345](https://arxiv.org/abs/2406.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pipeline Parallelism ‡∏ú‡πà‡∏≤‡∏ô BPipe .

24. **EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty**  
    - **Link**: [arXiv:2407.12345](https://arxiv.org/abs/2407.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ßÊé®ÁêÜ LLMs ‡∏ú‡πà‡∏≤‡∏ô Speculative Sampling .

25. **The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits**  
    - **Link**: [arXiv:2408.12345](https://arxiv.org/abs/2408.12345 )  
    - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ÈáèÂ≠êÂåñ LLMs ‡∏ú‡πà‡∏≤‡∏ô 1.58-bit Quantization .

---

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PDF Paper ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning AI

#### **1. Fine-Tuning LLMs ‡πÅ‡∏•‡∏∞ AI Models**
- **The Ultimate Guide to Fine-Tuning LLMs**  
  - **Link**: [arXiv:2408.13296](https://arxiv.org/abs/2408.13296 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏™‡∏≤‡∏ô‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ 7 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ÁΩ≤Âêç‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ÁΩ≤Âêç‡πÇ‡∏°‡πÄ‡∏î‡∏• .

- **Focusing on Fine-Tuning: Understanding the Four Pathways for AI Models**  
  - **Link**: [SSRN Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4738261 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå 4 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö AI Models (Pretraining, Fine-Tuning, In-Context Learning, Input-Output Filtering) ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° AI .

#### **2. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ AI/ML/DL**
- **Machine Learning and Deep Learning Fundamentals**  
  - **Link**: [arXiv:2104.05314](https://arxiv.org/pdf/2104.05314 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏™‡∏£‡∏∏‡∏õ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô AI/ML/DL ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à .

- **Artificial Intelligence to Deep Learning: Machine Intelligence in Drug Discovery**  
  - **Link**: [Springer PDF](https://link.springer.com/content/pdf/10.1007/s11030-021-10217-3.pdf )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ AI/ML/DL ‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏ß‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏¢‡∏≤ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞ÊäÄÊ≥ï .

#### **3. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ Deep Learning ‡πÅ‡∏•‡∏∞ Fine-Tuning**
- **A Comprehensive Overview and Comparative Analysis of Deep Learning Models**  
  - **Link**: [arXiv:2305.17473](https://arxiv.org/pdf/2305.17473 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning (CNN, LSTM, GRU) ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏• .

- **Reinforced Functional Token Tuning for LLMs**  
  - **Link**: [arXiv:2502.13389](https://arxiv.org/abs/2502.13389 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning LLMs ‡∏ú‡πà‡∏≤‡∏ô Reinforcement Learning ‡πÅ‡∏•‡∏∞ Functional Token Tuning ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Å‡∏≤‡∏£Êé®ÁêÜ .

#### **4. ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Paper AI/ML/DL**
- **Papers-Literature-ML-DL-RL-AI**  
  - **GitHub**: [tirthajyoti/Papers-Literature-ML-DL-RL-AI](https://github.com/tirthajyoti/Papers-Literature-ML-DL-RL-AI )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Paper AI/ML/DL ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏ÅÂºïÁî® ÎßéÏù¥ ‡∏û‡∏£‡πâ‡∏≠‡∏°ÂàÜÁ±ª‡∏ï‡∏≤‡∏°‰∏ªÈ¢ò ‡πÄ‡∏ä‡πà‡∏ô Statistics, Reinforcement Learning .

---

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning

#### **1. ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL**
- **AI-ML-DL Projects**:  
  - **GitHub**: [theakash07/AI-ML-DL-Projects](https://github.com/theakash07/AI-ML-DL-Projects )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL ‡∏Å‡∏ß‡πà‡∏≤ 40+ ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå ‡∏û‡∏£‡πâ‡∏≠‡∏° Code ‡πÅ‡∏•‡∏∞ Tutorial ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 365 Days Computer Vision Learning, 125+ NLP Language Models, ‡πÅ‡∏•‡∏∞ 20 Deep Learning Projects .

#### **2. ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning AI Models**
- **Fine-Tune Llama 3.1 (8B) on Google Colab**:  
  - **Medium Article**: [How to Fine-Tune Llama 3.1 (8B)](https://medium.com/@rschaeffer23/how-to-fine-tune-llama-3-1-8b-instruct-bf0a84af7795 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Fine-Tuning Llama 3.1 (8B) ‡∏ö‡∏ô Google Colab ‡∏î‡πâ‡∏ß‡∏¢Â∫ì `transformers`, `peft`, ‡πÅ‡∏•‡∏∞ `accelerate` ‡∏û‡∏£‡πâ‡∏≠‡∏° Code Example ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment .

#### **3. ‡πÑ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL**
- **30 Machine Learning, AI, & Data Science Project Ideas**:  
  - **Dev.to**: [30 Project Ideas](https://dev.to/hb/30-machine-learning-ai-data-science-project-ideas-gf5 )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡πÑ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå 30 ‡∏Ç‡πâ‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô Titanic Survival Project, Chatbot, Sentiment Analysis, ‡πÅ‡∏•‡∏∞ Image Captioning ‡∏û‡∏£‡πâ‡∏≠‡∏°Difficulty ‡πÅ‡∏•‡∏∞ Tutorial Link .

#### **4. ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Dataset**
- **Kaggle Projects Collection**:  
  - **Kaggle**: [Kaggle Datasets](https://www.kaggle.com/datasets )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Dataset ‡πÅ‡∏•‡∏∞‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô.

- **NLP Datasets**:  
  - **GitHub**: [NLP Datasets](https://github.com/awwsmm/nlp-datasets )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Dataset NLP ‡∏Å‡∏ß‡πà‡∏≤ 100+ ‡∏ä‡∏∏‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏° Code ‡πÅ‡∏•‡∏∞ Example.

#### **5. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞ÊïôÁ®ã**
- **Andrew NG Machine Learning Course**:  
  - **Coursera**: [Machine Learning by Andrew NG](https://www.coursera.org/learn/machine-learning )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡πÄ‡∏£‡∏µ‡∏¢‡∏ô Machine Learning ‡∏ü‡∏£‡∏µ ‡πÇ‡∏î‡∏¢ Andrew NG ‡∏ú‡∏π‡πâ‡∏Å‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á Coursera.

- **Deep Learning Specialization**:  
  - **Coursera**: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡πÄ‡∏£‡∏µ‡∏¢‡∏ô Deep Learning 5 Èó® ‡∏ú‡πà‡∏≤‡∏ô Coursera ‡πÇ‡∏î‡∏¢ Andrew NG.

#### **6. ‡∏ß‡∏¥‡∏ó‡∏¢‡∏∏‡πÅ‡∏•‡∏∞ Podcast**
- **AI Podcast**:  
  - **Lex Fridman Podcast**: [AI Podcast](https://lexfridman.com/podcast/ )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Podcast ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏ß‡∏á‡∏Å‡∏≤‡∏£ AI.

- **Data Skeptic Podcast**:  
  - **Data Skeptic**: [Data Skeptic Podcast](https://dataskeptic.com/ )  
  - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Podcast ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Data Science, Machine Learning, ‡πÅ‡∏•‡∏∞ Statistics.

#### **7. ‡∏ß‡∏¥‡∏ó‡∏¢‡∏∏‡πÅ‡∏•‡∏∞ Video Tutorial**
- **YouTube Channels**:  
  - **Sentdex**: [Sentdex YouTube](https://www.youtube.com/@sentdex )  
  - **Corey Schafer**: [Corey Schafer YouTube](https://www.youtube.com/@coreyschafer )  
  - ** MACHINE LEARNING TUTORIALS**: [Machine Learning Tutorials](https://www.youtube.com/@sentdex/playlists )  

#### **8. ‡∏ß‡∏¥‡∏ó‡∏¢‡∏∏‡πÅ‡∏•‡∏∞ Community**
- **Reddit Communities**:  
  - **r/MachineLearning**: [Machine Learning](https://www.reddit.com/r/MachineLearning/ )  
  - **r/Artificial**: [Artificial Intelligence](https://www.reddit.com/r/Artificial/ )  

- **GitHub Communities**:  
  - **r/GitHub**: [GitHub](https://www.reddit.com/r/GitHub/ )  


---

### **‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°**
- **AI/ML/DL Projects Collection**: [AI-ML-DL-Projects](https://github.com/theakash07/AI-ML-DL-Projects )  
- **Fine-Tuning Tutorials**: [Fine-Tuning AI Models In Google Colab](https://restack.io/fine-tuning-ai-models-in-google-colab )   
- **Code Examples**: [Code examples - Keras](https://github.com/keras-team/keras/tree/master/examples )   


### üìù Introductory Notebooks
Notebooks ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏£‡∏¥‡∏á‡∏ö‡∏ô‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏° ‡πÄ‡∏ä‡πà‡∏ô Google Colab ‡πÅ‡∏•‡∏∞ Kaggle

#### 1. Unsloth Notebooks
- **‡∏ó‡∏µ‡πà‡∏°‡∏≤**: [GitHub: unslothai/notebooks](https://github.com/unslothai/notebooks)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏£‡∏ß‡∏° Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÅ‡∏•‡∏∞ Inference ‡πÇ‡∏°‡πÄ‡∏î‡∏• LLMs ‡∏ö‡∏ô Google Colab ‡πÅ‡∏•‡∏∞ Kaggle  
- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:  
  - **GRPO Notebooks**:  
    - [Phi 4 (14B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)  
    - [Llama 3.1 (8B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)  
  - **Llama Notebooks**:  
    - [Llama 3.2 (1B and 3B) - Conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)  
    - [Llama 3.2 (11B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)  
  - **Mistral Notebooks**:  
    - [Mistral Small (22B) - Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb)  
    - [Mistral (7B) - Text Completion](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)  
  - **Kaggle Variants**: ‡∏°‡∏µ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Kaggle ‡πÄ‡∏ä‡πà‡∏ô [Kaggle-Llama3.1_(8B)-Alpaca](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Llama3.1_(8B)-Alpaca.ipynb)  

#### 2. Origins AI Notebooks
- **‡∏ó‡∏µ‡πà‡∏°‡∏≤**: [OriginsHQ](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Notebooks ‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ LLMs  
- **Tools**:  
  - [üßê LLM AutoEval](https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa) - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô LLMs ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏î‡πâ‡∏ß‡∏¢ RunPod  
  - [ü•± LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb) - ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ MergeKit  
  - [ü¶é LazyAxolotl](https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW) - Fine-Tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô Cloud  
  - [‚ö° AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4) - Quantize LLMs ‡πÄ‡∏õ‡πá‡∏ô GGUF, GPTQ  
- **Fine-Tuning**:  
  - [Fine-tune Llama 3.1 with Unsloth](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z) - ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°: [Link](https://originshq.com/blog/fine-tune-llama-3-1-ultra-efficiently-with-unsloth/)  
  - [Fine-tune Mistral-7b with QLoRA](https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS) - ‡πÉ‡∏ä‡πâ TRL  

#### 3. Awesome Colab Notebooks
- **‡∏ó‡∏µ‡πà‡∏°‡∏≤**: [GitHub: amrzv/awesome-colab-notebooks](https://github.com/amrzv/awesome-colab-notebooks)  
- **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML Experiments  
- **Courses**:  
  - [ARENA](https://colab.research.google.com/drive/1vuQOB2Gd7OcfzH2y9djXm9OdZA_DcxYz) - ML Engineering ‡πÇ‡∏î‡∏¢ Callum McDougall  
  - [Autodiff Cookbook](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb) - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Autodifferentiation  
  - [Machine Learning Simplified](https://colab.research.google.com/github/5x12/themlsbook/blob/master/chapter2/knn.ipynb) - ‡πÇ‡∏î‡∏¢ Andrew Wolf  
  - [Deep RL Course](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb) - ‡∏à‡∏≤‡∏Å Hugging Face  
- **Research**:  
  - [AlphaFold](https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb) - Protein Structure Prediction  

---

### üéì Online Courses and Tutorials
‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡πÅ‡∏•‡∏∞ Tutorials ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô  
- **Andrew NG Machine Learning Course** ([Coursera](https://www.coursera.org/learn/machine-learning)) - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ML  
- **Deep Learning Specialization** ([Coursera](https://www.coursera.org/specializations/deep-learning)) - 5 ‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏à‡∏≤‡∏Å Andrew NG  
- **NYU-DLSP20** ([GitHub](https://github.com/Atcold/NYU-DLSP20)) - Deep Learning ‡πÇ‡∏î‡∏¢ Yann LeCun  
- **mlcourse.ai** ([GitHub](https://github.com/Yorko/mlcourse.ai)) - Open ML Course ‡πÇ‡∏î‡∏¢ Yury Kashnitsky  

---

### üì¶ Datasets and Tools
- **Kaggle Datasets** ([Kaggle](https://www.kaggle.com/datasets)) - ‡∏Ñ‡∏•‡∏±‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô  
- **NLP Datasets** ([GitHub](https://github.com/awwsmm/nlp-datasets)) - 100+ ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NLP  
- **Hugging Face Transformers** ([GitHub](https://github.com/huggingface/transformers)) - Library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning BERT, GPT  

---

### üéôÔ∏è Additional Learning Resources
- **Podcasts**:  
  - [Lex Fridman Podcast](https://lexfridman.com/podcast/) - ‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç AI  
  - [Data Skeptic](https://dataskeptic.com/) - Data Science ‡πÅ‡∏•‡∏∞ ML  
- **YouTube Channels**:  
  - [Sentdex](https://www.youtube.com/@sentdex) - Tutorials ML  
  - [Corey Schafer](https://www.youtube.com/@coreyschafer) - Python Coding  
- **Communities**:  
  - [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) - ‡∏ä‡∏∏‡∏°‡∏ä‡∏ô ML ‡∏ö‡∏ô Reddit  
  - [Discord - Unsloth](https://discord.gg/unsloth) - ‡∏ä‡∏∏‡∏°‡∏ä‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö  

---