# Awesome AI/LLM Learning Resources for 2025

*‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025*  
*‡∏ó‡∏µ‡πà‡∏°‡∏≤: ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å [Unsloth Notebooks](https://github.com/unslothai/notebooks), [Awesome Colab Notebooks](https://github.com/amrzv/awesome-colab-notebooks), [Origins AI](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/), ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°*

‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ AI ‡πÅ‡∏•‡∏∞ Large Language Models (LLMs) ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏µ 2025 ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏™‡πà‡∏ß‡∏ô:

- **Part 1**: ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô AI/LLM ‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô  
- **Part 2**: ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ LLMs ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á (LLM Scientist)  
- **Part 3**: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ LLMs (LLM Engineer)  
- **Part 4**: GitHub Repositories ‡πÅ‡∏•‡∏∞ Scripts ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á  

---

## Part 1: LLM Fundamentals and Introductory Resources

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ AI ‡πÅ‡∏•‡∏∞ LLMs ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå, Python, Neural Networks ‡πÅ‡∏•‡∏∞ NLP

### üìö LLM Fundamentals

- **Mathematics for Machine Learning**  
  - *Linear Algebra*: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Derivatives, Integrals, Limits, Series, Multivariable Calculus, ‡πÅ‡∏•‡∏∞ Gradients  
  - *Probability and Statistics*: ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
  - **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
    - [Mathematics for ML](https://mml-book.github.io/) - ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ü‡∏£‡∏µ‡∏à‡∏≤‡∏Å Cambridge  
    - [3Blue1Brown Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) - ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏™‡∏≠‡∏ô  

- **Python for AI**  
  - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏ß‡∏¢ Python ‡πÅ‡∏•‡∏∞ libraries ‡πÄ‡∏ä‡πà‡∏ô NumPy, Pandas, Matplotlib  
  - **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
    - [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Jake VanderPlas  
    - [RealPython](https://realpython.com/) - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô Python  

- **Neural Networks**  
  - *‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô*: Layers, Weights, Biases, Activation Functions (Sigmoid, Tanh, ReLU)  
  - *‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•*: Backpropagation, Optimization  
  - **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
    - [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ü‡∏£‡∏µ‡πÇ‡∏î‡∏¢ Michael Nielsen  
    - [CS231n](https://cs231n.github.io/) - ‡∏Ñ‡∏≠‡∏£‡πå‡∏™ Stanford  

- **Natural Language Processing (NLP)**  
  - *Text Preprocessing*: Tokenization, Stemming, Lemmatization, Stop Word Removal  
  - *Feature Extraction*: Bag-of-Words, TF-IDF, N-grams  
  - *Word Embeddings*: Word2Vec, GloVe, FastText  
  - *RNNs*: LSTMs, GRUs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequential Data  
  - **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
    - [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html)  
    - [Jay Alammar - Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)  
    - [Colah‚Äôs Blog - Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  

### üìù Introductory Notebooks
- **Unsloth Notebooks** ([GitHub](https://github.com/unslothai/notebooks))  
  - *‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á*:  
    - [Llama 3.1 (8B) - Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)  
    - [Phi 4 - Conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)  
    - [Mistral (7B) - Text Completion](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)  

- **Origins AI Notebooks** ([OriginsHQ](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/))  
  - *Tools*:  
    - [LLM AutoEval](https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa) - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô LLMs ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥  
    - [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb) - ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏á‡πà‡∏≤‡∏¢ ‡πÜ  

- **Awesome Colab Notebooks** ([GitHub](https://github.com/amrzv/awesome-colab-notebooks))  
  - *Courses*:  
    - [ARENA](https://colab.research.google.com/drive/1vuQOB2Gd7OcfzH2y9djXm9OdZA_DcxYz) - ML Engineering ‡πÇ‡∏î‡∏¢ Callum McDougall  
    - [Autodiff Cookbook](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb) - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Autodiff  

---

## Part 2: The LLM Scientist - Advanced LLM Development

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á LLMs ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î

### üß† LLM Architecture
- *Overview*: Evolution ‡∏à‡∏≤‡∏Å Encoder-Decoder ‡∏™‡∏π‡πà Decoder-Only (‡πÄ‡∏ä‡πà‡∏ô GPT)  
- *Tokenization*: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç  
- *Attention Mechanisms*: Self-Attention, Long-Range Dependencies  
- *Sampling Techniques*: Greedy Search, Beam Search, Nucleus Sampling  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [3Blue1Brown - Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M)  
  - [Andrej Karpathy - nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)  

### ‚öôÔ∏è Pre-training Models
- *Data Preparation*: ‡πÉ‡∏ä‡πâ Dataset ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (‡πÄ‡∏ä‡πà‡∏ô Llama 3.1 ‡∏ù‡∏∂‡∏Å‡∏ö‡∏ô 15T tokens)  
- *Distributed Training*: Data Parallelism, Pipeline Parallelism, Tensor Parallelism  
- *Optimization*: AdamW, Mixed-Precision Training  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)  
  - [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2)  

### üìä Post-training Datasets
- *Storage*: ShareGPT, OpenAI/HF Formats  
- *Synthetic Data*: ‡πÉ‡∏ä‡πâ GPT-4o ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
- *Enhancement*: Chain-of-Thought, Auto-Evol  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LLM Datasets](https://github.com/mlabonne/llm-datasets)  
  - [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator)  

### üîß Supervised Fine-Tuning (SFT)
- *Techniques*: Full Fine-Tuning, LoRA, QLoRA  
- *Parameters*: Learning Rate, Batch Size, Epochs  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Fine-tune Llama 3.1 with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3)  
  - [Axolotl Docs](https://axolotl-ai-cloud.github.io/axolotl/)  

### üéØ Preference Alignment
- *Methods*: DPO, PPO, Rejection Sampling  
- *Monitoring*: Loss Curves, Accuracy  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [RLHF by Hugging Face](https://huggingface.co/blog/rlhf)  
  - [Fine-tune Mistral-7b with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)  

### üìà Evaluation
- *Automated Benchmarks*: MMLU  
- *Human Evaluation*: Arena Voting  
- *Model-based*: Judge Models  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)  
  - [Chatbot Arena](https://lmarena.ai/)  

### ‚ö° Quantization
- *Techniques*: GGUF, GPTQ, AWQ  
- *Tools*: llama.cpp, ExLlamaV2  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Introduction to Quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html)  
  - [4-bit Quantization with GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html)  

### üåü New Trends
- *Model Merging*: Mergekit (SLERP, DARE)  
- *Multimodal Models*: CLIP, LLaVA  
- *Interpretability*: Sparse Autoencoders  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Merge LLMs with Mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html)  
  - [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html)  

---

## Part 3: The LLM Engineer - Building LLM Applications

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ LLMs ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ Deploy

### üöÄ Running LLMs
- *APIs*: OpenAI, Hugging Face  
- *Local*: LM Studio, Ollama  
- *Prompt Engineering*: Zero-Shot, Few-Shot  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Prompt Engineering Guide](https://www.promptingguide.ai/)  
  - [Run LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)  

### üìÇ Building Vector Storage
- *Document Ingestion*: PDF, JSON  
- *Splitting*: Recursive Splitting  
- *Embedding Models*: Sentence Transformers  
- *Vector DBs*: Chroma, Pinecone  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LangChain - Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)  
  - [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)  

### üîç Retrieval Augmented Generation (RAG)
- *Orchestrators*: LangChain, LlamaIndex  
- *Retrievers*: Multi-Query, HyDE  
- *Evaluation*: Ragas, DeepEval  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LangChain - Q&A with RAG](https://python.langchain.com/docs/use_cases/question_answering/quickstart)  
  - [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/)  

### ‚öôÔ∏è Advanced RAG
- *Query Construction*: SQL, Cypher  
- *Agents*: Tool Selection (Google, Python)  
- *Post-Processing*: RAG-Fusion  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [LangChain - SQL](https://python.langchain.com/docs/use_cases/qa_structured/sql)  
  - [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task)  

### ‚ö° Inference Optimization
- *Flash Attention*: ‡∏•‡∏î Complexity  
- *Key-Value Cache*: MQA, GQA  
- *Speculative Decoding*: Draft + Refine  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Hugging Face - GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one)  
  - [Databricks - LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)  

### üåê Deploying LLMs
- *Local*: Ollama, oobabooga  
- *Demo*: Gradio, Streamlit  
- *Server*: TGI, vLLM  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [Streamlit - LLM App](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)  
  - [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm)  

### üîí Securing LLMs
- *Prompt Hacking*: Injection, Jailbreaking  
- *Defensive Measures*: Red Teaming  
- **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:  
  - [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)  
  - [LLM Security](https://llmsecurity.net/)  

---

## Part 4: GitHub Repositories and Advanced Scripts

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏°‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö GitHub ‡πÅ‡∏•‡∏∞ Scripts ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI/ML/DL ‡πÅ‡∏•‡∏∞ Fine-Tuning

### üìÇ GitHub Repositories
1. **Unsloth Notebooks**  
   - **GitHub**: [unslothai/notebooks](https://github.com/unslothai/notebooks)  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Notebooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏ä‡πà‡∏ô Llama, Phi, Mistral  
     - [Llama 3.1 (8B) - GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)  
     - [Qwen 2 VL (7B) - Vision](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)  

2. **Awesome Colab Notebooks**  
   - **GitHub**: [amrzv/awesome-colab-notebooks](https://github.com/amrzv/awesome-colab-notebooks)  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö Notebooks ML Experiments  
     - [AlphaFold](https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb)  
     - [DeepLabCut](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_maDLC_TrainNetwork_VideoAnalysis.ipynb)  

3. **AI-ML-DL Projects**  
   - **GitHub**: [theakash07/AI-ML-DL-Projects](https://github.com/theakash07/AI-ML-DL-Projects)  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: 40+ ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå AI/ML/DL  
     - [365 Days CV](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/365-Days-Computer-Vision-Learning)  
     - [125+ NLP Models](https://github.com/theakash07/AI-ML-DL-Projects/tree/main/125-NLP-Language-Models)  

4. **ml-systems-papers**  
   - **GitHub**: [byungsoo-oh/ml-systems-papers](https://github.com/byungsoo-oh/ml-systems-papers)  
   - **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: Paper ‡πÅ‡∏•‡∏∞ Scripts ‡∏à‡∏≤‡∏Å SOSP, NeurIPS  
     - [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor)  
     - [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain)  

### ‚öôÔ∏è Advanced Scripts
#### Fine-Tuning & Optimization
1. [FractalTensor](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SOSP24_FractalTensor) - Nested Data Parallelism  
2. [4D Parallelism](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_4D_Parallelism) - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß LLM Training  
3. [QLoRA Fine-Tuning](https://github.com/georgesung/llm_qlora/blob/main/train.py) - Fine-Tuning ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥  
4. [LoongTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_LoongTrain) - Long-Sequence LLMs  

#### Distributed Training
5. [Democratizing AI](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/SC24_GPU_Supercomputers) - GPU Supercomputers  
6. [TorchTitan](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_TorchTitan) - PyTorch Native Solution  
7. [DistTrain](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv24_DistTrain) - Disaggregated Training  

#### Advanced Applications
8. [DeepSpeed-Ulysses](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_DeepSpeed-Ulysses) - Long-Sequence Transformers  
9. [FLM-101B](https://github.com/byungsoo-oh/ml-systems-papers/tree/main/arXiv23_FLM-101B) - Fine-Tuning 101B ‡∏î‡πâ‡∏ß‡∏¢ $100K  

### üìú Research Papers
- **Fine-Tuning**:  
  - [QLoRA](https://arxiv.org/abs/2305.14314)  
  - [LongLoRA](https://github.com/dvlab-research/LongLoRA)  
- **Distributed Training**:  
  - [Democratizing AI](https://arxiv.org/abs/2409.12345)  
  - [TorchTitan](https://arxiv.org/abs/2409.12345)  

### üöÄ How to Use
1. **‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î**: ‡∏Ñ‡∏•‡∏¥‡∏Å "Code" > "Download ZIP" ‡∏ö‡∏ô GitHub  
2. **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á**: ‡∏£‡∏±‡∏ô `pip install -r requirements.txt`  
3. **‡∏£‡∏±‡∏ô**: ‡πÉ‡∏ä‡πâ `python script_name.py` ‡πÉ‡∏ô Terminal  

---

*‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏ó‡∏∏‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ AI/LLM ‡πÉ‡∏ô‡∏õ‡∏µ 2025!*