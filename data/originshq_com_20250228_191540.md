# Top AI/LLM Learning Resources in 2025

**Source:** [Origins AI Blog](https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/)

**Published:** January 18, 2025

**Author:** apoorvakumar169

This document provides a structured and categorized list of the AI/LLM learning resources mentioned in the Origins AI blog post "Top AI/LLM learning resource in 2025".  It aims to be a more easily navigable and readable version of the original content.

## Learning Resources

### 1. Foundational Concepts

#### 1.1 Linear Algebra

*   **3Blue1Brown â€“ The Essence of Linear Algebra:**  A visual and intuitive introduction to linear algebra concepts. [YouTube Playlist](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
*   **Immersive Linear Algebra:** An interactive, visual approach to learning linear algebra. [Website](https://immersivemath.com/ila/learnmore.html)
*   **Khan Academy â€“ Linear Algebra:** A comprehensive course covering linear algebra fundamentals. [Khan Academy](https://www.khanacademy.org/math/linear-algebra)

#### 1.2 Calculus

*   **Khan Academy â€“ Calculus:**  A complete course on single-variable calculus. [Khan Academy](https://www.khanacademy.org/math/calculus-1)

#### 1.3 Statistics and Probability

*   **StatQuest with Josh Starmer â€“ Statistics Fundamentals:**  Clear explanations of core statistical concepts. [YouTube Playlist](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9)
*   **AP Statistics Intuition by Ms Aerin:**  An intuitive approach to understanding AP Statistics concepts. [Medium List](https://automata88.medium.com/list/cacc224d5e7d)
*   **Khan Academy â€“ Probability and Statistics:**  A comprehensive course covering probability and statistics. [Khan Academy](https://www.khanacademy.org/math/statistics-probability)

### 2. Python Programming

*   **Real Python:**  A vast collection of Python tutorials and articles. [Website](https://realpython.com/)
*   **freeCodeCamp â€“ Learn Python:**  A full Python course for beginners. [YouTube](https://www.youtube.com/watch?v=rfscVS0vtbw)
*   **Python Data Science Handbook:**  A comprehensive guide to data science tools in Python. [Website](https://jakevdp.github.io/PythonDataScienceHandbook/)

### 3. Machine Learning

*   **freeCodeCamp â€“ Machine Learning for Everybody:**  A beginner-friendly introduction to machine learning. [YouTube](https://youtu.be/i_LwzRVP7bg)
*   **Udacity â€“ Intro to Machine Learning:**  A foundational course on machine learning concepts. [Udacity](https://www.udacity.com/course/intro-to-machine-learning--ud120)

### 4. Deep Learning

*   **3Blue1Brown â€“ But what is a Neural Network?:**  A visual explanation of neural networks. [YouTube](https://www.youtube.com/watch?v=aircAruvnKk)
*   **freeCodeCamp â€“ Deep Learning Crash Course:**  A concise introduction to deep learning. [YouTube](https://www.youtube.com/watch?v=VyWAvY2CF9c)
*   **Fast.ai â€“ Practical Deep Learning:**  A hands-on course focused on practical deep learning applications. [Fast.ai](https://course.fast.ai/)
*   **Patrick Loeber â€“ PyTorch Tutorials:**  A series of tutorials on using PyTorch for deep learning. [YouTube Playlist](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4)

### 5. Natural Language Processing (NLP)

#### 5.1 General NLP

*   **Lena Voita â€“ Word Embeddings:**  An explanation of word embeddings in NLP. [Website](https://lena-voita.github.io/nlp_course/word_embeddings.html)
*   **RealPython â€“ NLP with spaCy in Python:**  A practical guide to using spaCy for NLP tasks. [RealPython](https://realpython.com/natural-language-processing-spacy-python/)
*   **Kaggle â€“ NLP Guide:**  A guide to NLP concepts and techniques. [Kaggle](https://www.kaggle.com/learn-guide/natural-language-processing)

#### 5.2 Word Embeddings

*   **Jay Alammar â€“ The Illustrated Word2Vec:**  A visual explanation of the Word2Vec algorithm. [Blog Post](https://jalammar.github.io/illustrated-word2vec/)

#### 5.3 RNNs and LSTMs

*   **Jake Tae â€“ PyTorch RNN from Scratch:**  A tutorial on building RNNs from scratch in PyTorch. [Blog Post](https://jaketae.github.io/study/pytorch-rnn/)
*   **colahâ€™s blog â€“ Understanding LSTM Networks:**  A classic explanation of LSTM networks. [Blog Post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

### 6. Transformers and LLMs

#### 6.1 Introduction and Visualization

*   **Visual intro to Transformers:** [YouTube](https://www.youtube.com/watch?v=wjZofJX0v4M)
*   **LLM Visualization:** [Website](https://bbycroft.net/llm)
*   **nanoGPT:** [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY)
*   **Building GPT from scratch:** [YouTube](https://www.youtube.com/watch?v=zduSFxRajkE)

#### 6.2 Core Concepts

*   **Attention? Attention!:** [Blog Post](https://lilianweng.github.io/posts/2018-06-24-attention/)
*   **Decoding Strategies in LLMs:** [Blog Post](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)

#### 6.3 Papers

*   **Llama 3.1:** [arXiv](https://arxiv.org/abs/2307.09288)
*   **Distributed Training:** [arXiv](https://arxiv.org/abs/2407.20018)
*   **Auto-Evol:** [arXiv](https://arxiv.org/abs/2406.00770)
*   **Direct Preference Optimization:** [arXiv](https://arxiv.org/abs/2305.18290)
*   **Proximal Policy Optimization:** [arXiv](https://arxiv.org/abs/1707.06347)
*   **GPTQ:** [arXiv](https://arxiv.org/abs/2210.17323)
*   **AWQ:** [arXiv](https://arxiv.org/abs/2306.00978)
*   **DARE:** [arXiv](https://arxiv.org/abs/2311.03099)
*   **TIES:** [arXiv](https://arxiv.org/abs/2311.03099)
*   **HyDE:** [arXiv](https://arxiv.org/abs/2212.10496)
*   **Multi-Query Attention:** [arXiv](https://arxiv.org/abs/1911.02150)
*   **Grouped-Query Attention:** [arXiv](https://arxiv.org/abs/2305.13245)

#### 6.4 Tools and Libraries

*   **FineWeb:** [Hugging Face Space](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
*   **RedPajama v2:** [Website](https://www.together.ai/blog/redpajama-data-v2)
*   **nanotron:** [GitHub](https://github.com/huggingface/nanotron)
*   **SmolLM2:** [GitHub](https://github.com/huggingface/smollm)
*   **Parallel Training:** [PDF](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf)
*   **OLMo 2:** [Website](https://allenai.org/olmo)
*   **LLM360:** [Website](https://www.llm360.ai/)
*   **ðŸ’¾ LLM Datasets:** [GitHub](https://github.com/mlabonne/llm-datasets)
*   **Synthetic Data Generator:** [Hugging Face Space](https://huggingface.co/spaces/argilla/synthetic-data-generator)
*   **NeMo-Curator:** [GitHub](https://github.com/NVIDIA/NeMo-Curator)
*   **Distilabel:** [Website](https://distilabel.argilla.io/dev/sections/pipeline_samples/)
*   **Semhash:** [GitHub](https://github.com/MinishLab/semhash)
*   **Chat Template:** [Hugging Face Docs](https://huggingface.co/docs/transformers/main/en/chat_templating)
*   **Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth:** [Hugging Face Blog](https://huggingface.co/blog/mlabonne/sft-llama3)
*   **Axolotl â€“ Documentation:** [Website](https://axolotl-ai-cloud.github.io/axolotl/)
*   **Mastering LLMs:** [Website](https://parlance-labs.com/education/)
*   **LoRA insights:** [Website](https://lightning.ai/pages/community/lora-insights/)
*   **Illustrating RLHF:** [Hugging Face Blog](https://huggingface.co/blog/rlhf)
*   **LLM Training: RLHF and Its Alternatives:** [Sebastian Raschka Blog](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)
*   **Preference Tuning LLMs:** [Hugging Face Blog](https://huggingface.co/blog/pref-tuning)
*   **Fine-tune Mistral-7b with DPO:** [Blog Post](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)
*   **DPO Wandb logs:** [Website](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4)
*   **Evaluation Guidebook:** [GitHub](https://github.com/huggingface/evaluation-guidebook)
*   **Open LLM Leaderboard:** [Hugging Face Space](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
*   **Language Model Evaluation Harness:** [GitHub](https://github.com/EleutherAI/lm-evaluation-harness)
*   **Lighteval:** [GitHub](https://github.com/huggingface/lighteval)
*   **Chatbot Arena:** [Website](https://lmarena.ai/)
*   **llama.cpp:** [GitHub](https://github.com/ggerganov/llama.cpp)
*   **EXL2:** [GitHub](https://github.com/turboderp/exllamav2)
*   **Introduction to Quantization:** [Blog Post](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html)
*   **Quantize Llama models with llama.cpp:** [Blog Post](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)
*   **4-bit LLM Quantization with GPTQ:** [Blog Post](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html)
*   **Understanding Activation-Aware Weight Quantization:** [Medium](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8)
*   **SmoothQuant on Llama 2 7B:** [GitHub](https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_llama_demo.ipynb)
*   **DeepSpeed Model Compression:** [Website](https://www.deepspeed.ai/tutorials/model-compression/)
*   **mergekit:** [GitHub](https://github.com/cg123/mergekit)
*   **CLIP:** [Website](https://openai.com/research/clip)
*   **Stable Diffusion:** [Website](https://stability.ai/stable-image)
*   **LLaVA:** [Website](https://llava-vl.github.io/)
*   **Merge LLMs with mergekit:** [Blog Post](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html)
*   **Smol Vision:** [GitHub](https://github.com/merveenoyan/smol-vision)
*   **Large Multimodal Models:** [Blog Post](https://huyenchip.com/2023/10/10/multimodal.html)
*   **Uncensor any LLM with abliteration:** [Hugging Face Blog](https://huggingface.co/blog/mlabonne/abliteration)
*   **Intuitive Explanation of SAEs:** [Blog Post](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html)
*   **Scaling test-time compute:** [Hugging Face Space](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)

### 7. APIs and Services

*   **OpenAI:** [Website](https://platform.openai.com/)
*   **Google:** [Website](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)
*   **Anthropic:** [Website](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)
*   **Cohere:** [Website](https://docs.cohere.com/docs)
*   **OpenRouter:** [Website](https://openrouter.ai/)
*   **Hugging Face:** [Website](https://huggingface.co/inference-api)
*   **Together AI:** [Website](https://www.together.ai/)

### 8. Model Repositories

*   **Hugging Face Hub:** [Website](https://huggingface.co/models)
*   **Hugging Face Spaces:** [Website](https://chatgpt.com/)

### 9. Local LLM Tools

*   **LM Studio:** [Website](https://lmstudio.ai/)
*   **llama.cpp:** [GitHub](https://github.com/ggerganov/llama.cpp)
*   **Ollama:** [Website](https://ollama.ai/)

### 10. Prompt Engineering and Constrained Generation

*   **LMQL:** [Website](https://lmql.ai/)
*   **Outlines:** [GitHub](https://github.com/outlines-dev/outlines)
*   **Guidance:** [GitHub](https://github.com/guidance-ai/guidance)
*   **Run an LLM locally with LM Studio:** [Blog Post](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)
*   **Prompt engineering guide:** [Website](https://www.promptingguide.ai/)
*   **Outlines â€“ Quickstart:** [Website](https://outlines-dev.github.io/outlines/quickstart/)
*   **LMQL â€“ Overview:** [Website](https://lmql.ai/docs/language/overview.html)

### 11. Vector Databases

*   **Chroma:** [Website](https://www.trychroma.com/)
*   **Pinecone:** [Website](https://www.pinecone.io/)
*   **Milvus:** [Website](https://milvus.io/)
*   **FAISS:** [Website](https://faiss.ai/)
*   **Annoy:** [GitHub](https://github.com/spotify/annoy)
*   **LangChain â€“ Text splitters:** [Website](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
*   **Sentence Transformers library:** [Website](https://www.sbert.net/)
*   **MTEB Leaderboard:** [Hugging Face Space](https://huggingface.co/spaces/mteb/leaderboard)
*   **The Top 5 Vector Databases:** [Blog Post](https://www.datacamp.com/blog/the-top-5-vector-databases)

### 12. RAG and Frameworks

*   **LangChain:** [Website](https://python.langchain.com/docs/get_started/introduction)
*   **LlamaIndex:** [Website](https://docs.llamaindex.ai/en/stable/)
*   **FastRAG:** [GitHub](https://github.com/IntelLabs/fastRAG)
*   **Ragas:** [GitHub](https://github.com/explodinggradients/ragas/tree/main)
*   **DeepEval:** [GitHub](https://github.com/confident-ai/deepeval)
*   **Llamaindex â€“ High-level concepts:** [Website](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)
*   **Pinecone â€“ Retrieval Augmentation:** [Website](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/)
*   **LangChain â€“ Q&A with RAG:** [Website](https://python.langchain.com/docs/use_cases/question_answering/quickstart)
*   **LangChain â€“ Memory types:** [Website](https://python.langchain.com/docs/modules/memory/types/)
*   **RAG pipeline â€“ Metrics:** [Website](https://docs.ragas.io/en/stable/concepts/metrics/index.html)
*   **RAG-fusion:** [GitHub](https://github.com/Raudaschl/rag-fusion)
*   **DSPy:** [GitHub](https://github.com/stanfordnlp/dspy)
*   **LangChain â€“ Query Construction:** [Blog Post](https://blog.langchain.dev/query-construction/)
*   **LangChain â€“ SQL:** [Website](https://python.langchain.com/docs/use_cases/qa_structured/sql)

### 13. Agents

*   **Pinecone â€“ LLM agents:** [Website](https://www.pinecone.io/learn/series/langchain/langchain-agents/)
*   **LLM Powered Autonomous Agents:** [Blog Post](https://lilianweng.github.io/posts/2023-06-23-agent/)
*   **LangChain â€“ OpenAIâ€™s RAG:** [Blog Post](https://blog.langchain.dev/applying-openai-rag/)
*   **DSPy in 8 Steps:** [Website](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task)

### 14. Inference and Deployment

*   **GPU Inference:** [Hugging Face Docs](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one)
*   **LLM Inference:** [Blog Post](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)
*   **Optimizing LLMs for Speed and Memory:** [Hugging Face Docs](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization)
*   **Assisted Generation:** [Hugging Face Blog](https://huggingface.co/blog/assisted-generation)
*   **LM Studio:** [Website](https://lmstudio.ai/)
*   **Ollama:** [Website](https://ollama.ai/)
*   **oobabooga:** [GitHub](https://github.com/oobabooga/text-generation-webui)
*   **kobold.cpp:** [GitHub](https://github.com/LostRuins/koboldcpp)
*   **Gradio:** [Website](https://www.gradio.app/)
*   **Streamlit:** [Website](https://docs.streamlit.io/)
*   **Hugging Face Spaces:** [Website](https://huggingface.co/spaces)
*   **TGI:** [GitHub](https://github.com/huggingface/text-generation-inference)
*   **vLLM:** [GitHub](https://github.com/vllm-project/vllm/tree/main)
*   **MLC LLM:** [GitHub](https://github.com/mlc-ai/mlc-llm)
*   **mnn-llm:** [GitHub](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md)
*   **Streamlit â€“ Build a basic LLM app:** [Website](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)
*   **HF LLM Inference Container:** [Hugging Face Blog](https://huggingface.co/blog/sagemaker-huggingface-llm)
*   **Philschmid blog:** [Website](https://www.philschmid.de/)
*   **Optimizing Latency:** [Blog Post](https://hamel.dev/notes/llm/inference/03_inference.html)

### 15. Security

*   **garak:** [GitHub](https://github.com/leondz/garak)
*   **langfuse:** [GitHub](https://github.com/langfuse/langfuse)
*   **OWASP LLM Top 10:** [Website](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
*   **Prompt Injection Primer:** [GitHub](https://github.com/jthack/PIPE)
*   **LLM Security:** [Website](https://llmsecurity.net/)
*   **@llm\_sec:** [Twitter](https://twitter.com/llm_sec)
*   **Red teaming LLMs:** [Website](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming)
